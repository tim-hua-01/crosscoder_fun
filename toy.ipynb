{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'crosscoder_fun'\n",
      "/crosscoder_fun\n"
     ]
    }
   ],
   "source": [
    "%cd crosscoder_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 10795, 7239, 36005, 417, 897, 247, 16237, 10669, 32], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_mid_model.tokenizer(\"Does pythia not use a bos token?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m-deduped into HookedTransformer\n",
      "Loaded pretrained model EleutherAI/pythia-160m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from utils import *\n",
    "from trainer import Trainer\n",
    "# %%\n",
    "device = 'cuda:0'\n",
    "\n",
    "base_model = HookedTransformer.from_pretrained(\n",
    "    \"pythia-160m-deduped\", \n",
    "    device=device, \n",
    ")\n",
    "\n",
    "checkpoint_mid_model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-160m-deduped\", \n",
    "    device=device, \n",
    "    checkpoint_value = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "def compile_all_tokens(sequence_length=256, batch_size=512, max_batches=1000, device='cuda:0', save_dir=None):\n",
    "    \"\"\"\n",
    "    Iterates through the pile_dedup_sample dataset and compiles a new all_tokens tensor.\n",
    "    If a saved tensor exists at save_dir, loads that instead of recomputing.\n",
    "\n",
    "    Args:\n",
    "        sequence_length (int): The length of each token sequence.\n",
    "        batch_size (int): The number of sequences per batch.\n",
    "        max_batches (int, optional): The maximum number of batches to process. Defaults to None.\n",
    "        device (str): The device to store the tensor on.\n",
    "        save_dir (str, optional): Directory to save/load tensor from. If None, uses default name.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing all tokenized sequences.\n",
    "    \"\"\"\n",
    "    # Create default save path if none provided\n",
    "    if save_dir is None:\n",
    "        save_dir = f\"all_tokens_seq{sequence_length}_batch{batch_size}_max{max_batches}.pt\"\n",
    "    save_path = Path(save_dir)\n",
    "\n",
    "    # Check if tensor already exists\n",
    "    if save_path.exists():\n",
    "        print(f\"Loading existing tensor from {save_path}\")\n",
    "        return torch.load(save_path).to(device)\n",
    "\n",
    "    # Load dataset\n",
    "    pile_dedup_sample = load_dataset('EleutherAI/the_pile_deduplicated', streaming=True, split='train')\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-160m-deduped')\n",
    "\n",
    "    all_token_batches = []\n",
    "\n",
    "    current_batch = []\n",
    "    batch_count = 0\n",
    "\n",
    "    print(\"Compiling all_tokens tensor...\")\n",
    "    pbar = tqdm(total=max_batches)\n",
    "    for sample in pile_dedup_sample:\n",
    "        # Tokenize the text\n",
    "        tokens = tokenizer.encode(sample['text'])\n",
    "        \n",
    "        # Get as many sequence_length chunks as possible from tokens\n",
    "        for i in range(0, len(tokens) - sequence_length + 1, sequence_length):\n",
    "            current_batch.append(tokens[i:i + sequence_length])\n",
    "            \n",
    "            # If batch is full, add to all_token_batches\n",
    "            if len(current_batch) == batch_size:\n",
    "                batch_tensor = torch.tensor(current_batch, dtype=torch.int32)\n",
    "                all_token_batches.append(batch_tensor)\n",
    "                current_batch = []\n",
    "                batch_count += 1\n",
    "                pbar.update(1)\n",
    "                # Check if we've reached the maximum number of batches\n",
    "                if max_batches and batch_count >= max_batches:\n",
    "                    break\n",
    "        \n",
    "        # Break outer loop if we've hit max_batches\n",
    "        if max_batches and batch_count >= max_batches:\n",
    "            break\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Handle the last batch if it's not empty\n",
    "    if current_batch:\n",
    "        batch_tensor = torch.tensor(current_batch, dtype=torch.int32)\n",
    "        all_token_batches.append(batch_tensor)\n",
    "\n",
    "    # Concatenate all batches into a single tensor\n",
    "    all_tokens = torch.cat(all_token_batches, dim=0).to(device)\n",
    "\n",
    "    # Save tensor\n",
    "    print(f\"Saving tensor to {save_path}\")\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(all_tokens.cpu(), save_path)\n",
    "\n",
    "    print(f\"Compiled all_tokens tensor with shape: {all_tokens.shape}\")\n",
    "    return all_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing tensor from all_tokens_seq256_batch512_max1000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6151/1812725462.py:30: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actual_toks = compile_all_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512000, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#manually add a bos token 0 to the start of each sequence\n",
    "actual_toks2 = torch.cat([torch.zeros(actual_toks.shape[0], 1, device=actual_toks.device, dtype=torch.int32), actual_toks[:, :-1]], dim=1)\n",
    "actual_toks2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  1147,   310,  2218,    13,   285,  9262,    15,  1422,   476],\n",
       "        [    0,  4566,  1411,   752,  5606,   310,  6326,   281,   320,   751],\n",
       "        [    0, 47190,  2820,   281,  3523,   731,   432,  2509,   594,    15],\n",
       "        [    0,  1679,   715,   436,    27,   368,   403,  7063,   247,  2829],\n",
       "        [    0,  1475,    15,  1310, 27725,   432,   643,  2285,   403,  1475],\n",
       "        [    0,   812,  1056,   247,  4522,   293,  8023,  3492,  1996,    15],\n",
       "        [    0,  2216,    13,   309,  7402,   598,  1469,   342,  8489, 15958],\n",
       "        [    0,   187,   688,   253,   990,    13,   253,  2934,   273,  1469],\n",
       "        [    0,    13,   309,  7402,   598, 47348,   954,   273,   731,    13],\n",
       "        [    0,   273,  1841,   497,   417,  2218,   187,   187,  6014,   780]],\n",
       "       device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_toks2[0:10,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512000, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_toks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4566,  1411,   752,  5606,   310,  6326,   281,   320,   751,    15,\n",
       "         1310,   368,  1581,   253,  2608,   281,  2619,   849,   281, 23554,\n",
       "         1633,    13,   352,   457,    84,   417,  5606, 10542,  1108,   352,\n",
       "          457,    84,   253,  6425,   273, 17497,  2216,    13,   253,   269,\n",
       "          494, 23179,   407,  8869,  1346,   281, 11757,   253,  1077,  2934,\n",
       "          273,  5606,    15, 16688,   639,    79,  6932,   285,   247, 20840,\n",
       "         2320,  6656,    13,   326,   457,    84,   417,  1633,   326, 32089,\n",
       "          479,   253,   987,  1039,    15,   187,   187, 35914,    13,   619,\n",
       "         5962,   277,  4002,   785,   672, 18000,   752,   281,  2794,   369,\n",
       "          417,   342,   752,   309,  3078,   281,  2794,    13,   533,   342,\n",
       "          752,   309,   858,   417,    15,   309,  1904,   457,    85,   971,\n",
       "          281,  2794,   271,   773,   565, 46207,  2216,   668, 40022,   285,\n",
       "        47723,  1067,   352,  5606,    15,   187,   187,  1552,   310,   247,\n",
       "         1895,    13,   273,  2282,    13,  1046,   643, 12417,   386,   671,\n",
       "          574,   281,  2454,    15,  1244, 32721,   407,   253, 12028,  9262,\n",
       "           13,   417,  1142,  7303,   281,   789,  1475,   352,    15,   309,\n",
       "          457,    69,  1333,   253,   760,  1524,  2900,   369,   949,   253,\n",
       "          897,   273, 13345,  5438,    13, 10380,    15,  1893,  2080,    13,\n",
       "          309,  6468,   457,    85,  2326,   667,  5857,   970,   436,   387,\n",
       "          697,  5161, 30355,    15,   187,   187,  2422,   284,    13,   436,\n",
       "          310,   816,   247,   794,  7324,   285,   846,   247,  1223,   309,\n",
       "         4425,   417,   281,   320,   347,  7654,   342,   253,  2165,  2934,\n",
       "           13,   285,  4136,  4266,   281,  2619,  5913,   309,  1869,   651,\n",
       "          789,   562,    15,   187,   187,  3220,  3302,  2934,   369,   281,\n",
       "         2794,  1633,   835, 16457,  3597,   281, 23554,   281,   247,  1735,\n",
       "         1268,   533,   574,   690,  2238,   273], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_toks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_all_tokens2(sequence_length=256, batch_size=512, max_batches=1000, device='cuda:0', save_dir=None):\n",
    "    \"\"\"\n",
    "    Iterates through the pile_dedup_sample dataset and compiles a new all_tokens tensor.\n",
    "    If a saved tensor exists at save_dir, loads that instead of recomputing.\n",
    "\n",
    "    Args:\n",
    "        sequence_length (int): The length of each token sequence.\n",
    "        batch_size (int): The number of sequences per batch.\n",
    "        max_batches (int, optional): The maximum number of batches to process. Defaults to None.\n",
    "        device (str): The device to store the tensor on.\n",
    "        save_dir (str, optional): Directory to save/load tensor from. If None, uses default name.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing all tokenized sequences.\n",
    "    \"\"\"\n",
    "    # Create default save path if none provided\n",
    "    if save_dir is None:\n",
    "        save_dir = f\"all_tokens_seq{sequence_length}_batch{batch_size}_max{max_batches}.pt\"\n",
    "    save_path = Path(save_dir)\n",
    "\n",
    "    # Check if tensor already exists\n",
    "    if save_path.exists():\n",
    "        print(f\"Loading existing tensor from {save_path}\")\n",
    "        return torch.load(save_path).to(device)\n",
    "\n",
    "    # Load dataset\n",
    "    pile_dedup_sample = load_dataset('EleutherAI/the_pile_deduplicated', streaming=True, split='train')\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-160m-deduped')\n",
    "\n",
    "    all_token_batches = []\n",
    "\n",
    "    current_batch = []\n",
    "    batch_count = 0\n",
    "\n",
    "    print(\"Compiling all_tokens tensor...\")\n",
    "    pbar = tqdm(total=max_batches)\n",
    "    for sample in pile_dedup_sample:\n",
    "        # Tokenize the text\n",
    "        tokens = tokenizer.encode(sample['text'])\n",
    "        print(tokens[:10])\n",
    "        # Get as many sequence_length chunks as possible from tokens\n",
    "        for i in range(0, len(tokens) - sequence_length + 1, sequence_length):\n",
    "            current_batch.append(tokens[i:i + sequence_length])\n",
    "            \n",
    "            # If batch is full, add to all_token_batches\n",
    "            if len(current_batch) == batch_size:\n",
    "                batch_tensor = torch.tensor(current_batch, dtype=torch.int32)\n",
    "                all_token_batches.append(batch_tensor)\n",
    "                current_batch = []\n",
    "                batch_count += 1\n",
    "                pbar.update(1)\n",
    "                # Check if we've reached the maximum number of batches\n",
    "                if max_batches and batch_count >= max_batches:\n",
    "                    break\n",
    "        \n",
    "        # Break outer loop if we've hit max_batches\n",
    "        if max_batches and batch_count >= max_batches:\n",
    "            break\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Handle the last batch if it's not empty\n",
    "    if current_batch:\n",
    "        batch_tensor = torch.tensor(current_batch, dtype=torch.int32)\n",
    "        all_token_batches.append(batch_tensor)\n",
    "\n",
    "    # Concatenate all batches into a single tensor\n",
    "    all_tokens = torch.cat(all_token_batches, dim=0).to(device)\n",
    "\n",
    "    # Save tensor\n",
    "    print(f\"Saving tensor to {save_path}\")\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    #torch.save(all_tokens.cpu(), save_path)\n",
    "\n",
    "    print(f\"Compiled all_tokens tensor with shape: {all_tokens.shape}\")\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720805a6e9a745ca886819cb0eabd32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling all_tokens tensor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1147, 310, 2218, 13, 285, 9262, 15, 1422, 476, 1132]\n",
      "[46167, 27, 9838, 2254, 21811, 33584, 187, 187, 38859, 6138]\n",
      "[10572, 8481, 322, 5247, 401, 3647, 3976, 1336, 187, 187]\n",
      "[32197, 10168, 13, 10396, 1294, 4694, 285, 253, 29242, 366]\n",
      "[36751, 5464, 29961, 7885, 19544, 13, 23011, 13, 3382, 13372]\n",
      "[92, 187, 50276, 3, 71, 793, 24314, 1381, 3925, 13]\n",
      "[31484, 187, 187, 9710, 187, 187, 37, 5338, 342, 952]\n",
      "[75, 4985, 50, 327, 380, 4145, 46, 6807, 32, 187]\n",
      "[42, 1849, 6311, 253, 14164, 5809, 14013, 908, 275, 6700]\n",
      "[15271, 8808, 254, 9491, 187, 187, 15271, 8808, 254, 9491]\n",
      "[49579, 896, 247, 4804, 7663, 187, 187, 2866, 1466, 479]\n",
      "[10457, 619, 10438, 13, 309, 3597, 281, 9580, 347, 1199]\n",
      "[18681, 15, 29580, 16732, 313, 42, 14, 28889, 10, 588]\n",
      "[49, 4741, 27, 8966, 4632, 8698, 559, 247, 8461, 187]\n",
      "[510, 35247, 6480, 14, 15505, 390, 25115, 9606, 407, 22085]\n",
      "[12547, 27, 3599, 899, 12000, 8585, 187, 187, 25494, 1982]\n",
      "[50273, 510, 14568, 3927, 273, 253, 13338, 2111, 273, 9892]\n",
      "[510, 767, 5971, 2634, 44, 1100, 261, 6116, 2621, 65]\n",
      "[14594, 13698, 6422, 49569, 387, 7565, 187, 187, 11387, 27]\n",
      "[2214, 253, 9922, 7021, 13, 247, 499, 6231, 285, 11453]\n",
      "[10638, 187, 10638, 8283, 313, 68, 10, 4022, 26829, 3364]\n",
      "[27967, 1557, 323, 2255, 15, 48583, 6803, 285, 4323, 273]\n",
      "[3039, 21523, 90, 25498, 1669, 253, 2165, 342, 247, 1669]\n",
      "[7034, 13304, 11360, 3488, 951, 11224, 12333, 2728, 313, 2350]\n",
      "[34, 657, 1830, 54, 25509, 3915, 4037, 47, 2637, 27934]\n",
      "[3074, 17203, 36304, 187, 187, 510, 15146, 47423, 8255, 394]\n",
      "[4509, 253, 4260, 14, 28476, 434, 1919, 253, 2810, 273]\n",
      "[30431, 6421, 369, 44096, 281, 617, 320, 4396, 466, 20187]\n",
      "[19758, 1545, 50275, 19758, 809, 22082, 187, 7261, 78, 295]\n",
      "[68, 1945, 346, 4909, 14, 68, 526, 263, 386, 14]\n",
      "[42, 1694, 247, 11772, 598, 1067, 13, 309, 1694, 281]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 8/200 [00:05<01:43,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[187, 187, 15611, 9643, 187, 187, 4, 40604, 187, 187]\n",
      "[22396, 187, 187, 510, 8215, 310, 247, 3569, 985, 342]\n",
      "[7142, 3979, 23249, 27, 36388, 2716, 273, 512, 32846, 342]\n",
      "[33, 13982, 370, 8695, 33725, 27, 11188, 6905, 13, 87]\n",
      "[2697, 884, 15, 19, 428, 367, 8592, 1574, 37193, 16530]\n",
      "[510, 270, 386, 312, 6712, 16928, 273, 399, 35447, 13]\n",
      "[283, 48934, 422, 187, 283, 15684, 1214, 46998, 16, 15905]\n",
      "[18091, 272, 187, 187, 14102, 71, 15390, 3959, 247, 2491]\n",
      "[2347, 4031, 4448, 3599, 5830, 434, 686, 45, 16580, 8]\n",
      "[29, 15597, 2730, 22225, 31, 187, 29, 53, 43561, 31]\n",
      "[510, 3522, 310, 3809, 17682, 323, 776, 7203, 18240, 15]\n",
      "[2044, 3596, 426, 551, 187, 50274, 881, 27, 1777, 12290]\n",
      "[3220, 22885, 9420, 571, 18605, 374, 14, 33590, 1283, 187]\n",
      "[16395, 456, 327, 253, 611, 832, 267, 9409, 8451, 275]\n",
      "[55, 1079, 372, 418, 5738, 1108, 16363, 29700, 327, 7565]\n",
      "[25810, 14, 15288, 266, 8246, 187, 187, 688, 1340, 281]\n",
      "[510, 3166, 297, 28598, 313, 12731, 10, 187, 187, 22117]\n",
      "[2374, 4247, 337, 13, 4765, 13, 253, 954, 1534, 46626]\n",
      "[21, 5697, 323, 11138, 634, 299, 14, 32248, 41301, 37054]\n",
      "[11241, 27, 322, 658, 26417, 284, 303, 3227, 16246, 386]\n",
      "[19174, 407, 418, 37173, 324, 579, 301, 187, 187, 2906]\n",
      "[4, 17331, 27176, 4966, 64, 47913, 58, 64, 31577, 1703]\n",
      "[9, 66, 10, 7327, 187, 30731, 351, 3825, 273, 253]\n",
      "[52, 2421, 378, 3385, 571, 611, 6721, 14, 1449, 41]\n",
      "[51, 290, 91, 416, 19973, 3690, 15, 313, 14253, 55]\n",
      "[20576, 273, 36976, 247, 3426, 1873, 13, 812, 368, 4496]\n",
      "[4, 18, 187, 14344, 187, 25972, 1771, 21578, 17855, 187]\n",
      "[32868, 12664, 12726, 6029, 4058, 744, 33826, 849, 4302, 310]\n",
      "[4013, 1569, 69, 12506, 1569, 69, 2, 309, 1353, 11870]\n",
      "[18681, 1528, 28430, 1297, 22343, 552, 12761, 85, 485, 313]\n",
      "[2302, 643, 42906, 2567, 556, 247, 14200, 7990, 273, 5482]\n",
      "[3122, 187, 11, 21737, 281, 253, 14325, 9107, 6807, 313]\n",
      "[25897, 13, 3978, 337, 13, 4050, 187, 187, 41296, 16061]\n",
      "[50, 27, 187, 187, 34164, 19676, 4433, 342, 1345, 16]\n",
      "[19346, 10765, 310, 247, 2846, 273, 6793, 2892, 285, 4466]\n",
      "[52, 34049, 39509, 3964, 301, 3719, 187, 187, 1615, 465]\n",
      "[12547, 29041, 323, 8008, 187, 187, 23433, 407, 10354, 27]\n",
      "[35, 609, 2135, 40542, 46165, 12286, 5226, 187, 187, 3633]\n",
      "[510, 1055, 273, 247, 5899, 11012, 3733, 2086, 327, 253]\n",
      "[10638, 190, 187, 10638, 7532, 1242, 407, 11685, 22, 20653]\n",
      "[688, 253, 12244, 187, 187, 44982, 1615, 27498, 187, 187]\n",
      "[1672, 7426, 17078, 4683, 416, 613, 9182, 6039, 11987, 4325]\n",
      "[4, 13831, 64, 2703, 64, 34943, 27, 2032, 187, 187]\n",
      "[11735, 17782, 28991, 399, 11236, 13, 27840, 187, 187, 9034]\n",
      "[7573, 257, 23087, 6018, 25059, 23902, 16216, 10507, 617, 1781]\n",
      "[41225, 11405, 398, 23017, 6865, 13, 41371, 2726, 9107, 187]\n",
      "[42, 18029, 281, 436, 581, 13, 10658, 456, 407, 3771]\n",
      "[3220, 747, 4156, 13554, 1984, 310, 562, 1024, 27, 380]\n",
      "[31515, 316, 4742, 20580, 9792, 7121, 187, 187, 4553, 247]\n",
      "[34, 376, 1028, 8125, 47957, 9741, 610, 27, 3510, 273]\n",
      "[49, 25125, 2057, 247, 5678, 11103, 723, 708, 40604, 9067]\n",
      "[8262, 17814, 13, 187, 1552, 310, 619, 806, 1501, 327]\n",
      "[29, 9571, 3145, 28, 3665, 426, 313, 17, 470, 28]\n",
      "[42, 11476, 309, 1620, 1869, 309, 457, 69, 513, 667]\n",
      "[50, 27, 187, 187, 4120, 706, 12354, 337, 15, 18]\n",
      "[34, 1355, 2846, 275, 14890, 556, 2668, 271, 2250, 281]\n",
      "[4612, 1946, 16928, 1720, 9352, 12605, 293, 4530, 285, 4765]\n",
      "[23648, 273, 4734, 17183, 7340, 875, 247, 6041, 285, 767]\n",
      "[41127, 38512, 187, 187, 2391, 66, 1263, 273, 253, 1984]\n",
      "[41312, 380, 46096, 187, 187, 24573, 457, 84, 4782, 3072]\n",
      "[24881, 13, 309, 2206, 1066, 281, 2312, 342, 247, 1387]\n",
      "[17441, 187, 187, 510, 994, 263, 73, 989, 14031, 14488]\n",
      "[15187, 38972, 4683, 611, 579, 1063, 457, 84, 27020, 313]\n",
      "[24095, 4705, 367, 3204, 1905, 14820, 247, 2434, 3847, 8726]\n",
      "[6826, 896, 281, 1227, 11919, 16, 7614, 16, 87, 5267]\n",
      "[10708, 1269, 17013, 8906, 187, 187, 2948, 313, 187, 186]\n",
      "[13815, 38, 10320, 5625, 187, 187, 13815, 38, 10320, 5625]\n",
      "[17895, 968, 790, 4704, 281, 4533, 187, 187, 14191, 968]\n",
      "[605, 8283, 5307, 14, 14952, 27430, 32893, 968, 256, 15]\n",
      "[39, 15145, 273, 253, 22211, 17078, 27, 2006, 3628, 16]\n",
      "[19262, 678, 1514, 27, 4648, 187, 187, 510, 1563, 4648]\n",
      "[1231, 897, 14268, 281, 1918, 368, 253, 1682, 2793, 1896]\n",
      "[10708, 17908, 15, 14769, 13068, 15, 435, 4063, 375, 348]\n",
      "[15745, 3000, 313, 22122, 1801, 31448, 2908, 10, 23686, 479]\n",
      "[25302, 436, 20295, 1652, 36787, 6951, 432, 31336, 285, 659]\n",
      "[60, 6300, 249, 8969, 589, 48212, 708, 11007, 659, 522]\n",
      "[510, 1083, 273, 253, 30117, 14228, 187, 187, 3463, 187]\n",
      "[6930, 187, 475, 20378, 11148, 19, 310, 271, 6880, 323]\n",
      "[10851, 375, 953, 275, 634, 6022, 273, 7336, 16347, 2395]\n",
      "[46735, 48595, 387, 5682, 777, 797, 84, 187, 187, 46735]\n",
      "[4509, 337, 4163, 4765, 13, 253, 13662, 7454, 310, 44083]\n",
      "[510, 17257, 932, 27, 9001, 19101, 273, 9354, 187, 187]\n",
      "[424, 35, 28775, 424, 18526, 3299, 805, 57, 37648, 21856]\n",
      "[2347, 513, 309, 12921, 619, 6222, 32, 187, 187, 1394]\n",
      "[1394, 476, 1056, 271, 14286, 281, 2525, 342, 634, 18598]\n",
      "[8267, 4491, 556, 697, 1211, 5319, 285, 6095, 15, 1198]\n",
      "[14039, 14, 972, 32, 38692, 24371, 275, 253, 23873, 2791]\n",
      "[7093, 6703, 775, 11548, 476, 320, 634, 1682, 3331, 390]\n",
      "[187, 1976, 6192, 15, 2051, 15, 19, 69, 18836, 313]\n",
      "[29, 2974, 31, 187, 29, 2915, 31, 187, 29, 73]\n",
      "[34706, 90, 285, 2588, 80, 13, 496, 16806, 494, 2]\n",
      "[14458, 3128, 32724, 4240, 187, 187, 9, 12115, 10, 187]\n",
      "[15778, 3271, 3489, 16, 34656, 14, 5707, 52, 1400, 6971]\n",
      "[42, 10112, 5701, 285, 25799, 14226, 273, 619, 3888, 594]\n",
      "[33498, 9, 2350, 49, 14, 725, 583, 10, 21028, 313]\n",
      "[10708, 246, 76, 15, 680, 31244, 15, 13998, 1201, 15]\n",
      "[510, 3668, 273, 253, 4104, 867, 2237, 272, 273, 6037]\n",
      "[34, 20299, 327, 5427, 187, 187, 14324, 187, 187, 27642]\n",
      "[42, 3517, 25436, 326, 46147, 285, 309, 574, 11371, 387]\n",
      "[1145, 23389, 4968, 11849, 2670, 6888, 2822, 15753, 50022, 24618]\n",
      "[18170, 13902, 6022, 285, 253, 9943, 23119, 38008, 8386, 745]\n",
      "[50, 27, 187, 187, 4947, 280, 4632, 10898, 5559, 31140]\n",
      "[25647, 6317, 457, 7088, 25565, 3968, 562, 32182, 1882, 22303]\n",
      "[41, 357, 9314, 532, 45970, 390, 789, 32, 46096, 807]\n",
      "[40, 636, 5582, 12325, 323, 5352, 4680, 187, 187, 27270]\n",
      "[16395, 456, 275, 253, 15493, 28989, 10947, 273, 1457, 2816]\n",
      "[510, 399, 17693, 17842, 378, 10505, 285, 399, 19278, 36529]\n",
      "[37, 5240, 1479, 19298, 1216, 428, 23060, 7151, 1866, 9423]\n",
      "[12560, 4995, 285, 10396, 35015, 8216, 4163, 13, 3436, 4765]\n",
      "[10708, 389, 15, 9152, 15, 5423, 9349, 9784, 28, 187]\n",
      "[14277, 7229, 2715, 568, 18, 15, 17, 3, 9706, 568]\n",
      "[50, 27, 187, 187, 2513, 352, 3309, 281, 3334, 40534]\n",
      "[2042, 368, 390, 634, 11651, 1335, 346, 23593, 3, 4228]\n",
      "[42, 1353, 3965, 2119, 309, 1353, 6501, 755, 247, 31720]\n",
      "[5949, 11451, 187, 187, 41390, 7641, 8652, 13, 28351, 9966]\n",
      "[48099, 11210, 457, 85, 1014, 15335, 6138, 697, 19571, 5282]\n",
      "[40151, 7520, 13882, 15777, 20673, 275, 43113, 31500, 187, 187]\n",
      "[12486, 13140, 15913, 14964, 428, 3954, 31884, 272, 43667, 708]\n",
      "[187, 34307, 329, 15, 19, 69, 884, 1449, 313, 7199]\n",
      "[49348, 310, 253, 8215, 2586, 835, 954, 952, 3153, 275]\n",
      "[14592, 6327, 23541, 313, 45, 7282, 17364, 10, 1108, 380]\n",
      "[35854, 47072, 432, 355, 21592, 2695, 187, 187, 26, 27]\n",
      "[3122, 22902, 11890, 1835, 4118, 187, 475, 8283, 313, 68]\n",
      "[18658, 38654, 27, 187, 22117, 310, 4720, 275, 2120, 14284]\n",
      "[2512, 310, 247, 11727, 326, 2296, 326, 604, 1633, 310]\n",
      "[187, 35938, 55, 1042, 308, 15, 378, 3322, 18587, 13]\n",
      "[4013, 13, 378, 3288, 14, 41, 3288, 13, 18175, 187]\n",
      "[10282, 733, 187, 187, 10282, 733, 187, 187, 1989, 352]\n",
      "[605, 8283, 6157, 14, 15068, 6745, 15, 1876, 12484, 32997]\n",
      "[1231, 1024, 2430, 12960, 281, 6184, 1029, 6064, 7989, 1445]\n",
      "[50, 27, 187, 187, 14344, 9107, 323, 3512, 539, 15821]\n",
      "[36, 610, 412, 47849, 313, 35255, 5282, 10, 5037, 263]\n",
      "[45, 1807, 2666, 13, 10733, 818, 27, 22587, 351, 1994]\n",
      "[9770, 31140, 310, 247, 3289, 1021, 5191, 6850, 15, 5402]\n",
      "[18905, 8802, 187, 187, 16910, 589, 5308, 4913, 13, 5003]\n",
      "[1545, 4908, 8252, 1981, 14102, 21264, 3404, 8267, 13996, 9]\n",
      "[8, 2327, 7654, 5618, 187, 187, 2044, 277, 5844, 28]\n",
      "[9817, 1142, 2571, 309, 1849, 644, 6153, 247, 2969, 14741]\n",
      "[18905, 479, 327, 187, 187, 28549, 13, 3285, 3919, 4050]\n",
      "[510, 378, 17146, 20718, 12899, 3400, 247, 2159, 7622, 949]\n",
      "[261, 3058, 15, 6037, 1617, 398, 403, 20139, 2556, 281]\n",
      "[8602, 281, 320, 43526, 807, 2455, 13, 2296, 6365, 7454]\n",
      "[3246, 1372, 5236, 13, 6192, 15, 313, 56, 41404, 10]\n",
      "[11063, 322, 16580, 1373, 428, 4568, 47047, 13677, 187, 187]\n",
      "[968, 4628, 3864, 4787, 187, 187, 249, 253, 3668, 187]\n",
      "[10708, 4955, 15, 4793, 89, 15, 7267, 15, 4997, 15]\n",
      "[29, 2974, 31, 187, 186, 29, 2522, 31, 996, 186]\n",
      "[13510, 2652, 12086, 322, 4121, 187, 187, 8677, 614, 3740]\n",
      "[510, 3126, 9843, 13, 49765, 427, 14059, 251, 76, 21384]\n",
      "[301, 27, 277, 18858, 14, 3566, 1976, 33899, 1812, 187]\n",
      "[4, 3709, 654, 11209, 16, 8400, 68, 3424, 15, 73]\n",
      "[187, 20, 31107, 15, 2051, 15, 30610, 313, 25929, 10]\n",
      "[28014, 187, 187, 13019, 187, 187, 4553, 4314, 3038, 273]\n",
      "[510, 29324, 452, 4720, 11621, 4861, 762, 2233, 7759, 275]\n",
      "[1147, 3589, 457, 85, 1048, 3622, 326, 6176, 369, 2783]\n",
      "[11862, 37051, 187, 187, 1992, 7777, 923, 604, 12732, 310]\n",
      "[10282, 14815, 320, 6495, 12414, 253, 18544, 1064, 398, 187]\n",
      "[19008, 356, 4843, 274, 510, 6793, 16772, 4797, 3295, 275]\n",
      "[43, 14, 52, 28321, 1229, 14, 1093, 2756, 50254, 50267]\n",
      "[2598, 13, 1024, 326, 253, 1594, 6188, 556, 9071, 1882]\n",
      "[41, 531, 1105, 16954, 285, 50225, 17195, 403, 40637, 14]\n",
      "[187, 30467, 1893, 15, 19, 69, 721, 3566, 313, 12430]\n",
      "[12978, 466, 19027, 48702, 322, 437, 6282, 187, 187, 25698]\n",
      "[13921, 50254, 50254, 50254, 50263, 15228, 8541, 6413, 2111, 187]\n",
      "[42, 2714, 885, 275, 6153, 4451, 3349, 5823, 4311, 36232]\n",
      "[3463, 29313, 35971, 17112, 187, 187, 510, 23075, 14, 84]\n",
      "[2948, 551, 27740, 13, 496, 720, 13, 19832, 748, 432]\n",
      "[28266, 13, 2552, 2164, 13, 4332, 187, 187, 42, 574]\n",
      "[510, 19802, 2418, 2499, 28508, 2285, 1912, 1016, 273, 253]\n",
      "[15883, 4611, 2652, 316, 25742, 893, 2726, 743, 348, 729]\n",
      "[14277, 7229, 2715, 568, 18, 15, 17, 3, 9706, 568]\n",
      "[664, 4822, 1491, 670, 1457, 15023, 12079, 8458, 432, 47486]\n",
      "[38496, 187, 187, 28781, 13, 4397, 898, 13, 4748, 187]\n",
      "[21, 49039, 5271, 7737, 12151, 398, 7890, 34600, 2135, 187]\n",
      "[46250, 1044, 249, 427, 3163, 281, 6304, 27521, 442, 434]\n",
      "[71, 3843, 1159, 1082, 2490, 50276, 6790, 298, 28, 187]\n",
      "[24490, 16300, 6875, 12, 39, 2474, 4022, 187, 187, 3996]\n",
      "[45157, 380, 1457, 314, 88, 5797, 15, 13593, 90, 21872]\n",
      "[36673, 783, 2256, 534, 588, 320, 273, 954, 6349, 23075]\n",
      "[1628, 43, 1662, 251, 310, 629, 273, 3906, 1495, 285]\n",
      "[10708, 4955, 15, 13670, 33504, 15, 69, 536, 4530, 15]\n",
      "[28781, 13, 4596, 3436, 13, 4695, 187, 187, 42, 816]\n",
      "[17784, 398, 275, 42855, 13, 18671, 708, 11669, 33743, 641]\n",
      "[50, 27, 187, 187, 2347, 513, 368, 1067, 247, 25109]\n",
      "[2946, 2943, 323, 11271, 631, 285, 657, 8901, 32, 3856]\n",
      "[48182, 9175, 5405, 22277, 401, 4234, 28285, 187, 187, 510]\n",
      "[1628, 52, 37333, 2, 1621, 13, 322, 37333, 11602, 21994]\n",
      "[50, 27, 187, 187, 29193, 50, 281, 11198, 1005, 1057]\n",
      "[37, 38156, 10316, 275, 1361, 281, 2278, 2752, 254, 24362]\n",
      "[42, 1353, 7964, 1469, 281, 1007, 715, 436, 247, 2372]\n",
      "[46785, 3196, 15, 7618, 12673, 187, 187, 9034, 15, 7618]\n",
      "[17151, 25581, 74, 18521, 2434, 15, 15335, 10098, 12126, 5721]\n",
      "[74, 36, 24259, 854, 14, 12871, 555, 281, 11386, 387]\n",
      "[14219, 209, 7770, 25382, 6217, 5479, 6781, 187, 14219, 209]\n",
      "[11888, 247, 8388, 49514, 1509, 16949, 326, 1620, 6331, 15598]\n",
      "[510, 1895, 15, 187, 510, 1740, 10149, 387, 253, 5004]\n",
      "[46, 7839, 7648, 7779, 399, 839, 323, 253, 37177, 7648]\n",
      "[3140, 10069, 8893, 27, 374, 187, 38246, 27, 260, 23]\n",
      "[1276, 434, 747, 275, 436, 2715, 313, 18, 15, 23]\n",
      "[1552, 5311, 369, 8927, 247, 873, 273, 24233, 347, 247]\n",
      "[24881, 13, 7717, 23230, 10286, 4439, 616, 4022, 16, 7132]\n",
      "[3039, 634, 2218, 342, 253, 2165, 323, 427, 5830, 374]\n",
      "[17534, 973, 13, 436, 310, 417, 253, 806, 2926, 326]\n",
      "[50, 27, 187, 187, 17745, 279, 272, 767, 1930, 407]\n",
      "[34395, 187, 187, 20439, 187, 187, 3980, 12632, 187, 187]\n",
      "[29, 2948, 6740, 36587, 9784, 16, 2522, 15, 88, 7229]\n",
      "[9, 48265, 297, 4978, 10, 1905, 6745, 310, 25251, 247]\n",
      "[2513, 322, 57, 36, 4775, 473, 1748, 1170, 416, 2894]\n",
      "[39, 34416, 1284, 16386, 30254, 187, 187, 18, 15, 8452]\n",
      "[48943, 187, 187, 6067, 3625, 1673, 310, 873, 13, 1024]\n",
      "[29, 66, 3860, 568, 3614, 1358, 2700, 15, 9111, 28705]\n",
      "[28830, 2652, 13046, 15796, 9778, 275, 16111, 187, 187, 510]\n",
      "[42, 452, 15378, 619, 46104, 2395, 275, 619, 6813, 32754]\n",
      "[50254, 50256, 51, 8112, 947, 1703, 27, 20015, 7941, 54]\n",
      "[12978, 619, 39581, 320, 6885, 387, 353, 6453, 32, 187]\n",
      "[13422, 831, 20299, 187, 187, 28549, 13, 1884, 4162, 4104]\n",
      "[12442, 22683, 10364, 8438, 21860, 410, 3366, 1727, 48486, 4325]\n",
      "[187, 40804, 2145, 14, 361, 14, 25592, 14, 8887, 187]\n",
      "[6892, 727, 12286, 2135, 14, 13617, 20400, 187, 187, 36]\n",
      "[2948, 45675, 20, 187, 2948, 673, 187, 2948, 36743, 535]\n",
      "[25090, 428, 416, 3175, 53, 187, 53, 18, 428, 49092]\n",
      "[19, 28623, 27, 187, 187, 18812, 342, 1029, 20187, 3290]\n",
      "[41497, 14277, 7229, 2715, 568, 18, 15, 17, 3, 9706]\n",
      "[50, 27, 187, 187, 5430, 281, 41688, 1873, 3491, 187]\n",
      "[47339, 1267, 1139, 13, 427, 15, 58, 15, 1905, 380]\n",
      "[10708, 389, 15, 1866, 1154, 15, 2188, 15, 4113, 15]\n",
      "[17491, 253, 3048, 2708, 187, 187, 3404, 6926, 2360, 457]\n",
      "[4497, 13, 309, 1353, 14861, 15, 309, 3153, 275, 15543]\n",
      "[3122, 45180, 24629, 27, 330, 14788, 10334, 14, 3429, 27]\n",
      "[50, 27, 187, 187, 7992, 42807, 3468, 1566, 3301, 342]\n",
      "[37010, 982, 187, 187, 13690, 8367, 21667, 15, 3918, 10053]\n",
      "[605, 6307, 4561, 407, 564, 14, 2140, 7215, 28, 7953]\n",
      "[14324, 187, 187, 12995, 1359, 1992, 10727, 19566, 12070, 44981]\n",
      "[37517, 427, 459, 23210, 313, 487, 484, 287, 25947, 10]\n",
      "[26788, 13, 4223, 374, 13, 4267, 187, 187, 8, 29165]\n",
      "[510, 2256, 2647, 10557, 11249, 3839, 7033, 281, 271, 9954]\n",
      "[47074, 253, 378, 21597, 20189, 8154, 1641, 1066, 14331, 17503]\n",
      "[8997, 380, 22338, 187, 187, 22526, 49017, 2296, 8005, 812]\n",
      "[27363, 47025, 187, 187, 6300, 1119, 562, 670, 8462, 18527]\n",
      "[14214, 187, 187, 34435, 253, 17226, 342, 914, 46896, 2348]\n",
      "[27970, 14, 11523, 7875, 4320, 1025, 275, 6976, 407, 42935]\n",
      "[45, 10558, 267, 13, 18908, 13, 4397, 3349, 13, 4022]\n",
      "[510, 747, 18802, 23626, 444, 9889, 27, 4410, 832, 1699]\n",
      "[44617, 815, 11406, 479, 281, 2748, 326, 309, 5007, 281]\n",
      "[23108, 657, 21132, 12932, 358, 1170, 187, 187, 26788, 13]\n",
      "[41464, 50276, 12352, 27, 50276, 29, 2413, 1358, 81, 6434]\n",
      "[1909, 1023, 257, 275, 10049, 414, 12772, 434, 4223, 5403]\n",
      "[510, 388, 1519, 383, 19305, 17061, 35290, 641, 2530, 5312]\n",
      "[10708, 14118, 187, 187, 2044, 795, 2213, 907, 7456, 426]\n",
      "[5072, 796, 1485, 7068, 1080, 2098, 23159, 187, 187, 21756]\n",
      "[21691, 187, 187, 510, 747, 34442, 8098, 34340, 5986, 1608]\n",
      "[29146, 6340, 273, 9385, 4726, 273, 7532, 486, 428, 721]\n",
      "[3122, 187, 475, 308, 13932, 2548, 12804, 15, 5594, 187]\n",
      "[2042, 13, 751, 1142, 273, 253, 15350, 1063, 2285, 13]\n",
      "[10572, 253, 8246, 187, 187, 39965, 11788, 313, 10387, 2561]\n",
      "[19537, 187, 187, 34, 538, 662, 5259, 32286, 588, 10112]\n",
      "[9, 25182, 19824, 15, 681, 10, 428, 18673, 9533, 507]\n",
      "[43402, 2360, 818, 27, 1229, 2617, 27526, 4683, 4386, 12343]\n",
      "[4, 12425, 30, 17880, 14, 25, 187, 2948, 23629, 187]\n",
      "[8882, 14, 22045, 630, 187, 187, 688, 1340, 281, 320]\n",
      "[7510, 30387, 12989, 187, 187, 15689, 318, 807, 187, 187]\n",
      "[6199, 36974, 4965, 1070, 19896, 2652, 2345, 187, 187, 6199]\n",
      "[16490, 907, 90, 36657, 496, 187, 187, 16490, 907, 90]\n",
      "[510, 6318, 12672, 1049, 43862, 310, 31771, 187, 187, 2809]\n",
      "[6930, 1214, 3140, 2379, 50276, 4807, 293, 12639, 263, 8916]\n",
      "[28266, 13, 3307, 4565, 4748, 187, 187, 28531, 30208, 13]\n",
      "[8924, 31059, 457, 79, 457, 43956, 48771, 457, 281, 320]\n",
      "[49, 12586, 12517, 187, 187, 12092, 2, 28980, 281, 253]\n",
      "[50, 27, 187, 187, 51, 9986, 460, 691, 70, 656]\n",
      "[24004, 15, 20614, 537, 35193, 723, 23406, 6840, 347, 3000]\n",
      "[688, 22235, 407, 33322, 7235, 273, 47438, 273, 440, 18132]\n",
      "[36, 1028, 1777, 86, 543, 310, 1077, 11117, 275, 43079]\n",
      "[18139, 1482, 253, 25893, 6680, 875, 253, 5271, 706, 1502]\n",
      "[21408, 20186, 273, 33414, 6912, 187, 187, 49789, 287, 6912]\n",
      "[47, 438, 1346, 25752, 1066, 285, 19294, 3270, 14, 46723]\n",
      "[14324, 187, 187, 1276, 457, 84, 634, 7245, 58, 32]\n",
      "[60, 36, 614, 347, 2418, 362, 15, 30946, 529, 8950]\n",
      "[41497, 5302, 4155, 28, 187, 5302, 30073, 8224, 15, 25155]\n",
      "[3039, 2509, 19241, 281, 247, 5447, 352, 457, 84, 1077]\n",
      "[30952, 32735, 14271, 31, 187, 29, 2974, 19457, 568, 257]\n",
      "[38453, 2220, 253, 45200, 789, 273, 29876, 45902, 475, 292]\n",
      "[49926, 48, 27, 3173, 3018, 6864, 14, 24330, 14, 33491]\n",
      "[10708, 372, 15, 365, 1796, 82, 15, 88, 321, 296]\n",
      "[38496, 187, 187, 26788, 13, 5080, 3435, 13, 4240, 187]\n",
      "[41497, 48924, 10521, 187, 605, 654, 15149, 14, 20419, 31]\n",
      "[187, 24, 1508, 367, 15, 19, 69, 41267, 313, 14880]\n",
      "[30056, 10588, 26031, 187, 10588, 26031, 187, 5455, 3493, 7526]\n",
      "[39, 23144, 323, 247, 7952, 1818, 23848, 187, 187, 34]\n",
      "[14067, 247, 9734, 432, 15636, 12284, 4716, 17890, 417, 281]\n",
      "[42, 971, 281, 1056, 38240, 1445, 469, 7095, 323, 247]\n",
      "[2512, 310, 642, 14624, 253, 958, 326, 2360, 5333, 5820]\n",
      "[4257, 18733, 25729, 11405, 187, 187, 30098, 2189, 18733, 310]\n",
      "[3233, 3025, 11244, 1071, 187, 4531, 1395, 14, 9486, 14]\n",
      "[27399, 42, 476, 457, 85, 3890, 849, 6685, 13864, 359]\n",
      "[17919, 187, 187, 4257, 6089, 265, 2205, 10518, 411, 1103]\n",
      "[14277, 7229, 2715, 568, 18, 15, 17, 3, 23983, 187]\n",
      "[10572, 831, 16428, 187, 187, 22098, 22151, 854, 187, 187]\n",
      "[2808, 15, 5251, 49755, 2808, 15, 5251, 94, 187, 2808]\n",
      "[3122, 187, 475, 8283, 313, 68, 10, 4240, 13, 6247]\n",
      "[48031, 15, 8640, 2073, 47628, 15, 911, 10750, 15, 257]\n",
      "[7282, 805, 14, 52, 1093, 14, 54, 1348, 187, 187]\n",
      "[25643, 1142, 952, 403, 38611, 784, 281, 253, 5312, 26349]\n",
      "[510, 500, 284, 257, 729, 317, 2986, 2570, 14278, 273]\n",
      "[15545, 562, 273, 8742, 9757, 13, 359, 2714, 907, 275]\n",
      "[35638, 187, 187, 6189, 12579, 187, 187, 26788, 13, 4437]\n",
      "[2042, 368, 452, 1077, 4105, 6152, 13, 253, 45869, 1113]\n",
      "[510, 4477, 6583, 326, 512, 941, 6944, 253, 4342, 403]\n",
      "[50, 27, 187, 187, 2347, 8312, 310, 352, 281, 16497]\n",
      "[3122, 8283, 6247, 5559, 3690, 15, 1876, 12484, 32997, 15]\n",
      "[688, 25798, 4360, 1943, 19928, 407, 11327, 693, 255, 708]\n",
      "[45615, 1263, 4850, 5769, 10836, 457, 84, 4116, 187, 187]\n",
      "[8494, 6228, 27, 19348, 476, 564, 3076, 1014, 672, 16893]\n",
      "[20266, 1884, 13, 4332, 187, 187, 6407, 15129, 4245, 253]\n",
      "[3864, 74, 661, 300, 332, 23557, 2667, 489, 8432, 4204]\n",
      "[2513, 15066, 8492, 4270, 715, 530, 15, 52, 15, 13163]\n",
      "[41497, 605, 8283, 313, 68, 10, 9664, 7489, 20174, 13]\n",
      "[41239, 16, 7573, 257, 7225, 9585, 187, 187, 510, 714]\n",
      "[688, 10652, 7497, 13, 9980, 310, 247, 2201, 2847, 273]\n",
      "[50, 27, 187, 187, 2347, 281, 3334, 17633, 17203, 327]\n",
      "[25330, 2756, 187, 1042, 3003, 14732, 3481, 6246, 2894, 1042]\n",
      "[3633, 18261, 22774, 285, 2559, 49607, 15, 6618, 598, 13]\n",
      "[1532, 187, 7916, 27, 1650, 187, 5564, 27, 39979, 40185]\n",
      "[18744, 32952, 457, 84, 7823, 347, 1457, 2816, 3228, 14967]\n",
      "[510, 16146, 3975, 11032, 285, 16146, 3975, 8170, 6138, 1390]\n",
      "[29779, 323, 45464, 12963, 413, 20254, 17724, 187, 187, 18837]\n",
      "[187, 24, 367, 15, 20, 69, 7584, 313, 6914, 10]\n",
      "[48290, 33254, 778, 320, 2139, 3130, 37889, 2458, 1669, 41466]\n",
      "[5496, 27, 190, 187, 14, 470, 15, 18, 190, 187]\n",
      "[49, 1797, 52, 330, 25523, 25642, 367, 4740, 411, 991]\n",
      "[510, 1986, 2077, 285, 4047, 588, 3213, 598, 616, 17147]\n",
      "[51, 23502, 30292, 187, 187, 26626, 342, 3253, 432, 7126]\n",
      "[34, 6304, 7556, 10834, 281, 15185, 253, 25853, 27498, 831]\n",
      "[5232, 3627, 337, 4050, 7648, 21578, 13525, 11099, 5578, 10850]\n",
      "[1909, 634, 3367, 1720, 2711, 263, 13, 309, 588, 2770]\n",
      "[11185, 3443, 1002, 262, 187, 187, 2115, 265, 342, 247]\n",
      "[1394, 403, 1060, 27, 187, 187, 4374, 309, 1353, 247]\n",
      "[36569, 989, 7756, 275, 2903, 312, 4112, 187, 187, 8693]\n",
      "[14324, 187, 187, 510, 20574, 273, 31303, 187, 187, 688]\n",
      "[39453, 39664, 428, 40046, 17251, 187, 187, 39453, 39664, 499]\n",
      "[47, 4237, 41733, 556, 4102, 34800, 697, 747, 5579, 14423]\n",
      "[46, 4881, 12295, 5112, 8481, 36244, 187, 187, 6300, 1897]\n",
      "[2347, 18770, 9876, 352, 12805, 187, 187, 2214, 1110, 417]\n",
      "[510, 17068, 3448, 273, 9550, 310, 48526, 2584, 18000, 285]\n",
      "[1552, 3102, 310, 752, 359, 1067, 253, 4780, 8063, 27]\n",
      "[4, 34834, 4805, 16, 31333, 187, 4, 411, 9816, 33069]\n",
      "[13641, 24959, 2822, 444, 1634, 3964, 4867, 81, 13, 11356]\n",
      "[1508, 27, 1229, 13, 1458, 4565, 4748, 25193, 15, 17809]\n",
      "[35638, 281, 253, 11573, 13, 7233, 25144, 187, 187, 6407]\n",
      "[18686, 2239, 1982, 884, 14, 2913, 24209, 11026, 403, 6507]\n",
      "[52, 3592, 2146, 380, 1497, 708, 5143, 434, 15574, 187]\n",
      "[7845, 8046, 34619, 281, 1132, 436, 3492, 6283, 15, 1310]\n",
      "[42, 1158, 634, 2285, 310, 14999, 1612, 15, 35469, 273]\n",
      "[11586, 11491, 187, 187, 510, 1007, 327, 353, 303, 457]\n",
      "[12323, 8439, 27, 13198, 25742, 322, 9367, 2752, 7813, 187]\n",
      "[20431, 330, 2272, 187, 187, 2214, 1781, 11839, 4893, 13]\n",
      "[688, 6426, 187, 187, 34181, 3489, 21318, 3252, 7400, 187]\n",
      "[1532, 43608, 16, 42446, 15, 442, 186, 6755, 14, 883]\n",
      "[52, 5009, 18312, 264, 40632, 285, 2884, 609, 9067, 4202]\n",
      "[1552, 2523, 369, 3863, 275, 767, 9508, 13, 2596, 407]\n",
      "[187, 187, 26106, 706, 1162, 46377, 1997, 12784, 187, 187]\n",
      "[4967, 8958, 634, 4096, 26866, 368, 594, 1199, 673, 275]\n",
      "[14305, 47, 34659, 1830, 27, 380, 6440, 8414, 273, 247]\n",
      "[510, 23061, 443, 1556, 14942, 58, 19660, 818, 15, 17]\n",
      "[9112, 991, 2726, 11204, 1435, 187, 187, 9, 21, 10]\n",
      "[36, 11177, 34174, 403, 1929, 323, 4177, 1612, 533, 597]\n",
      "[41, 857, 27356, 434, 13063, 585, 66, 27, 9979, 13]\n",
      "[605, 831, 2603, 1873, 310, 629, 273, 253, 24619, 15]\n",
      "[4045, 844, 18741, 27, 2802, 1231, 12238, 14136, 3824, 16275]\n",
      "[31900, 556, 5262, 2030, 1107, 275, 253, 3153, 14608, 4491]\n",
      "[2042, 368, 452, 644, 7270, 275, 253, 4320, 15464, 273]\n",
      "[6560, 251, 380, 36024, 23895, 323, 370, 2313, 34211, 309]\n",
      "[1231, 476, 626, 3343, 323, 253, 11417, 18165, 436, 6926]\n",
      "[33466, 447, 327, 253, 47286, 4422, 187, 187, 3463, 970]\n",
      "[39, 23699, 8, 7284, 5952, 281, 3667, 4187, 473, 38190]\n",
      "[28781, 13, 4437, 721, 13, 4267, 187, 187, 688, 6758]\n",
      "[41, 29963, 1608, 272, 868, 187, 187, 41, 29963, 1608]\n",
      "[3152, 17638, 13, 19381, 1346, 13, 285, 8081, 24751, 187]\n",
      "[11212, 598, 323, 776, 25144, 187, 187, 28412, 275, 5181]\n",
      "[1036, 1706, 21043, 2385, 2385, 520, 1012, 746, 3944, 12985]\n",
      "[187, 15781, 21894, 15, 33152, 313, 30488, 10, 187, 1843]\n",
      "[40, 6182, 8305, 317, 11902, 84, 390, 4878, 68, 1792]\n",
      "[15041, 12432, 4187, 428, 3166, 3105, 3656, 4063, 187, 187]\n",
      "[12515, 272, 1416, 611, 4631, 66, 187, 187, 44, 2284]\n",
      "[11837, 10648, 31945, 2608, 310, 7922, 247, 2990, 2395, 285]\n",
      "[186, 6999, 3306, 481, 2038, 9, 3701, 1082, 551, 187]\n",
      "[688, 3668, 6281, 670, 253, 5974, 6735, 8891, 13, 359]\n",
      "[510, 1533, 457, 84, 806, 1021, 5191, 3647, 1824, 9509]\n",
      "[1231, 1472, 6212, 708, 41367, 434, 19489, 187, 187, 510]\n",
      "[10708, 1650, 15, 7645, 28, 187, 187, 2948, 24296, 15]\n",
      "[38537, 20299, 1063, 27, 329, 6416, 22042, 251, 8056, 7567]\n",
      "[12547, 23728, 27, 270, 6741, 4978, 187, 187, 1231, 403]\n",
      "[688, 1850, 1339, 91, 1866, 500, 24452, 2721, 13107, 1866]\n",
      "[1797, 1720, 3743, 6049, 844, 47836, 1500, 392, 251, 15859]\n",
      "[20990, 332, 41790, 6282, 6639, 8942, 187, 187, 32969, 42410]\n",
      "[7093, 12269, 323, 16942, 416, 9051, 5990, 187, 187, 13030]\n",
      "[3474, 11632, 18, 426, 1159, 1082, 25425, 187, 187, 3474]\n",
      "[187, 938, 20626, 15, 2051, 15, 374, 69, 38520, 313]\n",
      "[4, 3709, 346, 6682, 64, 5397, 15, 73, 3, 187]\n",
      "[34, 22840, 1094, 11572, 9833, 342, 625, 685, 370, 10695]\n",
      "[46, 5629, 33665, 3964, 19925, 432, 521, 1211, 4975, 20123]\n",
      "[50254, 50276, 48949, 2111, 273, 9892, 187, 50254, 50270, 19346]\n",
      "[8207, 466, 13, 1621, 885, 13, 29264, 909, 8879, 2866]\n",
      "[1545, 4908, 45, 1369, 1767, 77, 22093, 7100, 2115, 9]\n",
      "[510, 5882, 369, 11420, 275, 8210, 407, 6911, 500, 15]\n",
      "[34, 10020, 14, 30344, 22378, 327, 4782, 285, 2448, 8672]\n",
      "[688, 23718, 273, 10547, 14, 10516, 398, 327, 253, 275]\n",
      "[27346, 4554, 15716, 7103, 2720, 281, 21075, 21068, 7259, 2684]\n",
      "[5518, 3027, 19638, 273, 11937, 34022, 15, 187, 18310, 19215]\n",
      "[42, 1158, 253, 1524, 2523, 1060, 310, 326, 954, 952]\n",
      "[2948, 20279, 432, 686, 69, 6156, 16, 24717, 5618, 187]\n",
      "[23375, 310, 12060, 407, 285, 326, 2097, 253, 990, 273]\n",
      "[4756, 330, 3180, 10, 6028, 64, 9872, 16676, 254, 187]\n",
      "[49320, 899, 13, 6112, 1969, 12933, 899, 309, 3871, 556]\n",
      "[34, 747, 10463, 8461, 5936, 326, 25913, 443, 4696, 13]\n",
      "[21381, 71, 15053, 14494, 12680, 4250, 285, 2624, 2067, 11385]\n",
      "[9817, 3101, 13, 751, 6122, 187, 187, 8835, 1433, 10452]\n",
      "[4779, 465, 2477, 85, 1578, 27, 27766, 277, 20440, 5920]\n",
      "[44, 11413, 39184, 14, 37, 1227, 39184, 14, 37, 71]\n",
      "[49, 27306, 963, 1940, 251, 5171, 7532, 4595, 35560, 271]\n",
      "[9807, 15032, 275, 9664, 7454, 247, 41702, 187, 187, 510]\n",
      "[14918, 792, 187, 187, 18489, 2030, 13, 4748, 187, 187]\n",
      "[6504, 39575, 187, 187, 338, 368, 497, 275, 271, 734]\n",
      "[187, 187, 15035, 1511, 30073, 12236, 426, 187, 50274, 13570]\n",
      "[2320, 1102, 6317, 13, 3349, 4437, 428, 2726, 253, 15428]\n",
      "[6930, 187, 475, 21737, 762, 253, 14325, 4637, 13, 11099]\n",
      "[45, 4966, 1267, 41059, 187, 187, 26216, 38666, 34379, 13525]\n",
      "[18497, 23722, 3487, 327, 38415, 10787, 187, 187, 1311, 18071]\n",
      "[9707, 1165, 416, 9466, 52, 9280, 8158, 7043, 2413, 1358]\n",
      "[30377, 1261, 9940, 37419, 665, 452, 644, 16092, 6890, 253]\n",
      "[10708, 17908, 15, 46310, 316, 15, 262, 15, 4029, 2679]\n",
      "[36205, 4348, 470, 15, 21, 15, 25, 428, 844, 878]\n",
      "[510, 10498, 273, 253, 42234, 3130, 11550, 312, 1346, 187]\n",
      "[187, 28459, 4378, 15, 40191, 313, 39511, 10, 187, 2251]\n",
      "[4257, 24938, 436, 1770, 15, 754, 556, 3542, 323, 388]\n",
      "[42, 878, 690, 1361, 16901, 2, 378, 483, 619, 385]\n",
      "[1552, 2670, 310, 11658, 407, 247, 2136, 390, 9341, 9633]\n",
      "[3, 5648, 3123, 8, 52, 388, 35064, 13242, 33069, 3]\n",
      "[50, 27, 187, 187, 688, 2831, 12820, 13, 2169, 253]\n",
      "[1552, 310, 253, 806, 1388, 273, 776, 3565, 8667, 323]\n",
      "[510, 11879, 327, 1716, 75, 10593, 400, 5895, 313, 49]\n",
      "[1779, 2610, 6831, 5006, 3520, 84, 187, 187, 8389, 718]\n",
      "[187, 44030, 427, 15, 38, 15, 19, 69, 46148, 313]\n",
      "[6394, 4853, 346, 18867, 16, 4826, 3, 12377, 187, 187]\n",
      "[510, 4430, 14910, 273, 2255, 665, 6940, 818, 13, 933]\n",
      "[14277, 5581, 187, 6930, 187, 475, 6813, 32754, 187, 475]\n",
      "[11, 551, 187, 50274, 15456, 27, 470, 28, 187, 50274]\n",
      "[2327, 11775, 459, 64, 30872, 64, 22214, 64, 6443, 1450]\n",
      "[4, 10030, 4697, 8929, 187, 187, 60, 3138, 8893, 880]\n",
      "[35, 43975, 309, 19166, 187, 187, 11, 1879, 10892, 275]\n",
      "[12558, 32143, 275, 253, 18324, 273, 8784, 437, 2610, 13]\n",
      "[38642, 457, 84, 1682, 2021, 7512, 13977, 187, 187, 1231]\n",
      "[28549, 13, 5080, 374, 13, 4240, 187, 187, 4045, 368]\n",
      "[30952, 32735, 14271, 35283, 19479, 605, 56, 20, 36, 605]\n",
      "[8497, 27, 1310, 634, 4468, 310, 670, 20404, 272, 13]\n",
      "[998, 1467, 314, 8288, 13, 436, 285, 253, 17691, 5370]\n",
      "[29627, 1528, 1127, 7496, 19787, 1299, 19980, 556, 4783, 432]\n",
      "[4, 3709, 346, 7992, 3460, 10976, 17219, 4241, 15, 73]\n",
      "[40779, 273, 754, 4052, 342, 15366, 302, 1479, 253, 8081]\n",
      "[23852, 3277, 304, 2205, 1040, 14943, 9491, 1982, 187, 187]\n",
      "[4967, 31382, 1557, 310, 1774, 187, 187, 21914, 253, 1711]\n",
      "[49, 522, 900, 187, 187, 49, 522, 900, 187, 187]\n",
      "[6744, 13784, 310, 247, 13100, 14, 35, 7283, 14, 881]\n",
      "[998, 1225, 10840, 187, 187, 3039, 309, 369, 1469, 896]\n",
      "[3404, 21398, 90, 6258, 31349, 368, 588, 1089, 187, 2577]\n",
      "[10971, 3095, 858, 352, 2, 1380, 22487, 3528, 52, 20492]\n",
      "[56, 885, 36671, 187, 187, 3, 32244, 878, 417, 452]\n",
      "[28781, 13, 4596, 1884, 13, 4332, 187, 187, 510, 1563]\n",
      "[14918, 792, 49901, 2706, 187, 187, 510, 20186, 952, 21758]\n",
      "[2861, 404, 420, 273, 714, 583, 78, 614, 3289, 1024]\n",
      "[4149, 27, 686, 19, 8, 187, 187, 21922, 27, 187]\n",
      "[6892, 1832, 14934, 27, 608, 49039, 281, 12138, 1137, 3419]\n",
      "[14277, 5581, 187, 187, 13017, 966, 1777, 357, 695, 1080]\n",
      "[31535, 473, 9852, 621, 285, 44913, 187, 187, 1628, 510]\n",
      "[2925, 7310, 1982, 187, 187, 510, 34587, 989, 7465, 273]\n",
      "[55, 11430, 2024, 18731, 3277, 478, 187, 187, 510, 3975]\n",
      "[42, 8282, 4744, 262, 313, 47, 284, 1473, 82, 27]\n",
      "[20, 1107, 9, 969, 10, 187, 187, 14060, 655, 13]\n",
      "[140, 101, 5126, 5790, 32255, 6467, 7065, 19985, 10161, 10743]\n",
      "[1147, 4850, 7197, 15, 2091, 2790, 1299, 3047, 253, 5406]\n",
      "[1552, 2469, 7948, 13, 309, 574, 767, 7512, 12154, 9355]\n",
      "[1276, 513, 359, 5730, 954, 323, 776, 2151, 32, 10209]\n",
      "[47595, 390, 20389, 21695, 15073, 281, 380, 36044, 10840, 6317]\n",
      "[29694, 1895, 27, 329, 15722, 273, 2151, 187, 187, 24221]\n",
      "[28014, 84, 8, 4588, 7563, 4916, 30909, 187, 187, 34]\n",
      "[33, 13088, 745, 187, 187, 1747, 30123, 346, 34650, 22047]\n",
      "[1552, 310, 247, 9769, 12786, 16, 2437, 2250, 11953, 4576]\n",
      "[8693, 8216, 387, 43878, 14, 1773, 13, 3262, 408, 48777]\n",
      "[23631, 6057, 627, 403, 305, 698, 285, 3293, 67, 2458]\n",
      "[29617, 187, 187, 17588, 253, 14275, 273, 29310, 275, 16609]\n",
      "[605, 8283, 6585, 14, 9638, 34712, 34968, 38230, 1227, 34712]\n",
      "[21096, 281, 253, 1682, 36635, 40839, 2670, 327, 253, 8573]\n",
      "[10572, 12899, 43639, 187, 187, 3074, 254, 1247, 23895, 773]\n",
      "[510, 43648, 273, 2051, 14, 15545, 18805, 5489, 187, 187]\n",
      "[4553, 368, 5206, 634, 2143, 13, 253, 1735, 954, 1774]\n",
      "[28549, 13, 4565, 2145, 13, 4050, 187, 187, 16212, 1170]\n",
      "[19583, 4722, 436, 11959, 10584, 2216, 310, 253, 906, 273]\n",
      "[41497, 6930, 2379, 475, 50275, 20968, 313, 68, 10, 5612]\n",
      "[25989, 10111, 634, 9011, 74, 19241, 323, 598, 281, 767]\n",
      "[55, 1479, 13473, 367, 38498, 729, 187, 187, 55, 333]\n",
      "[13425, 187, 187, 47, 3227, 1535, 606, 310, 247, 26634]\n",
      "[25438, 15615, 3847, 12318, 8737, 27, 13209, 447, 323, 7289]\n",
      "[1231, 403, 9049, 326, 368, 588, 320, 16362, 253, 21661]\n",
      "[4, 3182, 32342, 1277, 4072, 190, 187, 4, 3182, 12229]\n",
      "[3476, 4107, 27, 317, 428, 1959, 9797, 24660, 187, 50276]\n",
      "[36945, 18544, 437, 3771, 1281, 7335, 532, 6758, 1594, 16090]\n",
      "[14067, 521, 3332, 4320, 19408, 13, 48048, 11085, 57, 2959]\n",
      "[21164, 422, 22607, 89, 187, 187, 42, 717, 271, 9558]\n",
      "[56, 80, 56, 308, 647, 4263, 16398, 337, 14, 10487]\n",
      "[1532, 1924, 15, 68, 15, 28474, 186, 8971, 14, 2904]\n",
      "[11592, 14, 267, 5905, 3422, 14126, 6728, 2728, 13, 6365]\n",
      "[11796, 11232, 9260, 187, 187, 11148, 1271, 13, 10679, 285]\n",
      "[42, 1053, 626, 452, 3367, 2793, 281, 8162, 533, 846]\n",
      "[1394, 403, 7967, 1063, 685, 5402, 743, 9487, 187, 187]\n",
      "[32, 20383, 20955, 13003, 1108, 380, 7126, 2216, 273, 776]\n",
      "[2347, 513, 309, 3523, 619, 4370, 6400, 272, 387, 7137]\n",
      "[6180, 187, 1552, 13911, 310, 3839, 2905, 281, 4382, 6928]\n",
      "[58, 19834, 25961, 25195, 1959, 187, 187, 24226, 2610, 1959]\n",
      "[3207, 18004, 1905, 329, 5603, 275, 15319, 326, 588, 3167]\n",
      "[21958, 13591, 281, 7489, 327, 27199, 187, 187, 510, 1048]\n",
      "[2948, 551, 8347, 12919, 6802, 5521, 748, 432, 37994, 32375]\n",
      "[35, 14375, 1556, 11156, 13, 4416, 818, 313, 32680, 10]\n",
      "[510, 45788, 4759, 19607, 13, 48475, 1481, 6056, 13, 12413]\n",
      "[18237, 23691, 187, 187, 39418, 253, 2554, 273, 8969, 2577]\n",
      "[2374, 253, 6010, 3239, 273, 253, 7984, 375, 4657, 352]\n",
      "[1909, 368, 871, 13, 33099, 434, 6258, 310, 3551, 3517]\n",
      "[34, 5878, 273, 4102, 2624, 285, 1335, 1652, 908, 11693]\n",
      "[50, 27, 187, 187, 9395, 11737, 1868, 273, 313, 21838]\n",
      "[15684, 686, 4946, 76, 11155, 16, 7404, 641, 16, 4658]\n",
      "[36, 11177, 32243, 35623, 3586, 18898, 457, 6258, 275, 253]\n",
      "[7420, 14, 19895, 187, 187, 510, 3061, 281, 15142, 247]\n",
      "[8398, 368, 323, 13975, 15392, 434, 21483, 272, 7088, 84]\n",
      "[47, 304, 5169, 6132, 2133, 34852, 3565, 6273, 1543, 187]\n",
      "[3650, 1929, 37351, 11376, 11369, 12714, 1361, 187, 187, 14324]\n",
      "[60, 664, 999, 89, 62, 187, 1590, 30, 664, 999]\n",
      "[3837, 2254, 980, 275, 1146, 247, 20848, 21424, 281, 3819]\n",
      "[11387, 21666, 187, 187, 1779, 19656, 11255, 7852, 27, 40433]\n",
      "[23779, 1020, 5511, 34865, 265, 327, 7562, 3540, 273, 665]\n",
      "[25353, 554, 729, 5858, 631, 15, 681, 3262, 656, 187]\n",
      "[39, 386, 3258, 37622, 6681, 3120, 1535, 32175, 428, 2389]\n",
      "[23536, 273, 16418, 5766, 3184, 327, 16075, 33254, 275, 7600]\n",
      "[45032, 18088, 428, 35512, 14, 35, 12586, 556, 187, 22337]\n",
      "[33669, 4276, 21254, 187, 187, 14909, 16261, 13, 5403, 9182]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 12/200 [00:07<01:34,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1992, 3048, 281, 253, 2862, 1789, 13, 15596, 436, 3048]\n",
      "[510, 40602, 326, 2186, 253, 33852, 4430, 327, 403, 1638]\n",
      "[605, 187, 605, 50276, 40, 11927, 1658, 3364, 31483, 5443]\n",
      "[510, 7899, 6120, 476, 320, 247, 6422, 12215, 323, 253]\n",
      "[4505, 15, 5758, 3914, 92, 18, 27, 3967, 64, 936]\n",
      "[1845, 14843, 13, 26281, 13, 3522, 285, 4948, 323, 416]\n",
      "[9709, 652, 29151, 24466, 27, 380, 15300, 54, 43667, 13]\n",
      "[8207, 9038, 407, 2451, 1616, 1880, 253, 6167, 4991, 310]\n",
      "[22, 15, 6049, 858, 2305, 15, 19498, 45035, 3599, 249]\n",
      "[18968, 29012, 1720, 30345, 14307, 347, 1457, 2434, 14, 21689]\n",
      "[5, 25410, 5731, 187, 5, 25410, 3334, 1121, 312, 187]\n",
      "[16, 69, 1641, 14, 87, 18, 16, 28, 187, 187]\n",
      "[26788, 13, 3285, 4223, 4332, 187, 187, 37701, 1658, 7736]\n",
      "[688, 14, 14996, 3492, 12089, 310, 253, 747, 2806, 15]\n",
      "[34, 4804, 1659, 281, 6431, 6849, 13, 642, 2647, 849]\n",
      "[34, 5579, 10651, 13, 2305, 18456, 18847, 285, 247, 3513]\n",
      "[28, 187, 28, 23709, 1873, 273, 6246, 32857, 24227, 52]\n",
      "[35947, 29019, 273, 11271, 12113, 13, 20859, 38793, 13, 285]\n",
      "[21822, 353, 3056, 6272, 6975, 10417, 14, 14660, 26733, 10168]\n",
      "[1628, 510, 11067, 34211, 46693, 3083, 668, 369, 581, 273]\n",
      "[21096, 187, 187, 25897, 13, 4565, 3127, 13, 4765, 187]\n",
      "[28781, 13, 4565, 1249, 13, 4765, 187, 187, 48, 22104]\n",
      "[56, 297, 561, 26476, 11802, 4173, 418, 8960, 611, 31066]\n",
      "[28007, 13, 5080, 655, 13, 4332, 187, 187, 2577, 401]\n",
      "[31853, 30944, 5827, 247, 2120, 6224, 8027, 275, 4765, 327]\n",
      "[605, 187, 605, 50276, 53, 11252, 9604, 15, 78, 187]\n",
      "[5993, 46, 13303, 27950, 12778, 484, 6618, 323, 3821, 15]\n",
      "[44293, 3346, 844, 878, 281, 7569, 253, 1234, 86, 1860]\n",
      "[29617, 187, 187, 28253, 187, 187, 14569, 310, 253, 1388]\n",
      "[586, 42051, 5562, 1744, 21167, 10906, 5562, 575, 2302, 15]\n",
      "[510, 47080, 434, 20308, 427, 3272, 26578, 187, 187, 27303]\n",
      "[27223, 3128, 18072, 250, 480, 7193, 323, 3192, 49364, 265]\n",
      "[3122, 187, 475, 8283, 313, 36, 10, 4059, 9997, 18444]\n",
      "[23433, 1621, 27, 8319, 187, 187, 1394, 403, 6684, 247]\n",
      "[46, 5629, 378, 3355, 14573, 27, 5057, 7648, 495, 17878]\n",
      "[42, 816, 1239, 253, 2926, 273, 247, 5006, 39380, 10145]\n",
      "[24391, 28063, 2866, 187, 187, 36601, 429, 309, 9919, 281]\n",
      "[92, 187, 50276, 3, 881, 1381, 346, 18886, 12517, 27]\n",
      "[13437, 1969, 14769, 247, 16, 13587, 5288, 270, 16, 13587]\n",
      "[44, 780, 66, 418, 1765, 321, 187, 187, 38, 255]\n",
      "[5302, 41945, 28, 187, 187, 12361, 45226, 12168, 51, 5902]\n",
      "[605, 8283, 313, 68, 10, 4072, 380, 26595, 1514, 26940]\n",
      "[12864, 247, 16419, 5253, 285, 7522, 253, 5004, 342, 581]\n",
      "[29596, 273, 6911, 416, 666, 5756, 4978, 434, 8024, 833]\n",
      "[20197, 62, 7727, 285, 743, 5741, 330, 1278, 968, 3656]\n",
      "[21831, 702, 5887, 376, 1608, 13347, 388, 8403, 376, 12682]\n",
      "[42, 1849, 816, 5262, 253, 1390, 4314, 1897, 327, 247]\n",
      "[4, 34834, 4805, 16, 31333, 187, 187, 338, 544, 428]\n",
      "[47, 1238, 66, 17202, 13, 533, 4105, 11319, 13, 14230]\n",
      "[30045, 187, 187, 36, 736, 69, 6290, 466, 2510, 26304]\n",
      "[53, 290, 800, 20579, 2368, 399, 684, 6618, 187, 187]\n",
      "[27031, 10462, 21483, 45551, 27014, 19761, 4461, 43504, 428, 831]\n",
      "[3122, 187, 50276, 1552, 18491, 310, 642, 3356, 15257, 8838]\n",
      "[16212, 1170, 6317, 187, 187, 3074, 34546, 47721, 35970, 187]\n",
      "[6300, 1918, 441, 247, 1067, 387, 47822, 755, 308, 4280]\n",
      "[17557, 1479, 187, 187, 46, 7397, 187, 187, 4, 1348]\n",
      "[42, 1266, 90, 22351, 1079, 66, 15, 316, 310, 417]\n",
      "[9795, 647, 398, 574, 7197, 6224, 275, 4596, 685, 3264]\n",
      "[10572, 3189, 187, 187, 3039, 369, 253, 1390, 673, 368]\n",
      "[40461, 3646, 27, 831, 2670, 4648, 14268, 313, 6795, 4367]\n",
      "[6930, 187, 475, 8283, 313, 68, 10, 4022, 14, 15068]\n",
      "[25897, 13, 3978, 1283, 13, 4267, 187, 187, 14651, 90]\n",
      "[1010, 3978, 15956, 1166, 1905, 1384, 27, 361, 187, 187]\n",
      "[2042, 952, 403, 11339, 2217, 326, 597, 878, 247, 1569]\n",
      "[11735, 262, 9858, 9497, 507, 27, 7890, 1605, 2713, 9858]\n",
      "[14219, 187, 50276, 95, 8283, 6247, 15, 5559, 16169, 187]\n",
      "[39270, 7399, 686, 78, 6133, 386, 6657, 8, 5979, 1479]\n",
      "[4, 6037, 15507, 22484, 23716, 2490, 187, 65, 81, 395]\n",
      "[510, 2448, 8273, 323, 12761, 36415, 313, 45352, 10, 310]\n",
      "[1785, 27, 23698, 2434, 47500, 16026, 2304, 281, 4235, 2050]\n",
      "[6436, 13, 16109, 2980, 253, 12315, 369, 253, 4975, 12321]\n",
      "[55, 12099, 412, 13000, 187, 187, 688, 534, 776, 22271]\n",
      "[2115, 37787, 30011, 4355, 1061, 1825, 300, 2433, 591, 24786]\n",
      "[1394, 403, 1060, 187, 187, 36, 304, 17395, 6224, 25760]\n",
      "[6648, 4683, 4693, 4567, 45663, 84, 3532, 39895, 84, 496]\n",
      "[28007, 13, 3285, 4437, 4022, 187, 187, 1898, 625, 327]\n",
      "[43254, 13, 3452, 531, 2985, 806, 1388, 273, 12949, 14]\n",
      "[50, 27, 187, 187, 5430, 281, 4459, 247, 15896, 3120]\n",
      "[38997, 16225, 457, 84, 3055, 11600, 281, 1527, 187, 187]\n",
      "[50, 27, 187, 187, 2347, 281, 3653, 253, 7162, 273]\n",
      "[8207, 23907, 432, 39193, 3058, 187, 187, 18596, 12937, 800]\n",
      "[187, 49909, 322, 15, 38, 15, 19, 69, 19092, 313]\n",
      "[17831, 8224, 187, 187, 11185, 187, 187, 17831, 4779, 1432]\n",
      "[49085, 378, 15, 16732, 13, 773, 14044, 264, 5034, 50072]\n",
      "[2447, 15870, 17335, 323, 21136, 27, 187, 187, 10342, 4633]\n",
      "[39, 383, 422, 2129, 275, 285, 562, 187, 41937, 187]\n",
      "[41803, 273, 7903, 36592, 708, 6235, 1498, 27120, 187, 187]\n",
      "[7371, 48735, 443, 628, 27417, 1680, 31960, 1198, 5402, 15694]\n",
      "[13030, 8910, 187, 187, 28907, 314, 23728, 27, 5080, 9169]\n",
      "[510, 1375, 434, 8127, 9922, 46866, 13, 9904, 342, 17966]\n",
      "[15348, 457, 84, 7066, 936, 251, 622, 11269, 697, 26465]\n",
      "[29, 2974, 31, 187, 187, 29, 2522, 31, 187, 29]\n",
      "[42, 717, 3240, 2119, 326, 672, 359, 4089, 273, 34722]\n",
      "[15992, 3171, 187, 187, 6104, 782, 22096, 15889, 850, 10315]\n",
      "[19537, 27, 17437, 14, 36122, 187, 187, 1552, 7102, 32636]\n",
      "[3251, 7022, 15836, 275, 28323, 4559, 1877, 4051, 706, 595]\n",
      "[1628, 1231, 403, 342, 779, 275, 247, 1077, 47644, 552]\n",
      "[3848, 75, 267, 2413, 1358, 615, 75, 267, 15, 2061]\n",
      "[46, 17894, 16441, 6317, 13, 17234, 8075, 13, 13126, 31349]\n",
      "[455, 253, 1491, 13, 5293, 273, 253, 26246, 1040, 1794]\n",
      "[14660, 43161, 187, 187, 28830, 10427, 466, 388, 2804, 187]\n",
      "[27302, 5660, 37947, 30494, 187, 187, 688, 9662, 457, 84]\n",
      "[50, 27, 187, 187, 2347, 281, 2289, 14137, 643, 685]\n",
      "[41497, 29, 11695, 29806, 8893, 568, 20, 15, 22, 3]\n",
      "[47, 4905, 19415, 33123, 310, 581, 273, 253, 1682, 18075]\n",
      "[1466, 479, 806, 10112, 368, 281, 253, 41732, 28100, 24703]\n",
      "[510, 5083, 310, 13, 1014, 2167, 16363, 3243, 588, 320]\n",
      "[50219, 21890, 947, 187, 187, 50219, 21890, 947, 187, 187]\n",
      "[47565, 7382, 9868, 187, 187, 47, 16128, 32254, 12098, 24471]\n",
      "[11538, 2679, 1108, 773, 36, 18568, 24415, 668, 313, 2503]\n",
      "[41497, 12361, 20378, 494, 4241, 18551, 38413, 190, 187, 92]\n",
      "[34934, 23728, 27, 4437, 1458, 13, 4695, 187, 187, 3220]\n",
      "[510, 3645, 6022, 33536, 35842, 4156, 7452, 285, 24482, 22599]\n",
      "[5429, 8863, 187, 1042, 3003, 14732, 3481, 6246, 2894, 1042]\n",
      "[11478, 2196, 187, 187, 1051, 395, 597, 1160, 271, 5400]\n",
      "[13443, 8339, 187, 187, 19258, 598, 634, 1495, 15, 5815]\n",
      "[14277, 7229, 2715, 568, 18, 15, 17, 3, 40468, 568]\n",
      "[6930, 187, 475, 8283, 313, 68, 10, 4104, 14, 15068]\n",
      "[13422, 187, 27958, 3009, 4793, 187, 187, 51, 357, 447]\n",
      "[510, 21672, 273, 3660, 17937, 187, 187, 14947, 4363, 50072]\n",
      "[605, 8283, 4059, 380, 26595, 1514, 26940, 15, 1876, 3570]\n",
      "[187, 3156, 1893, 15, 20, 69, 22856, 313, 7199, 10]\n",
      "[33832, 32521, 6317, 187, 187, 2354, 1556, 8728, 16319, 13]\n",
      "[6, 41, 2277, 56, 37862, 3660, 81, 2460, 407, 271]\n",
      "[36234, 1974, 11981, 187, 187, 21333, 16500, 26396, 13, 275]\n",
      "[2042, 634, 22485, 323, 247, 26603, 320, 2119, 281, 2486]\n",
      "[43, 28830, 2517, 1108, 7813, 23899, 15, 31633, 313, 21082]\n",
      "[39, 4877, 17274, 1634, 3599, 22947, 366, 323, 12493, 342]\n",
      "[40353, 2892, 13, 5427, 434, 806, 23355, 5442, 31043, 7517]\n",
      "[605, 187, 605, 50276, 30928, 77, 1334, 15, 78, 187]\n",
      "[45, 26537, 9109, 187, 187, 16582, 54, 27, 187, 187]\n",
      "[49027, 2073, 4305, 7152, 16, 7614, 16, 505, 6910, 1383]\n",
      "[1394, 403, 1060, 27, 187, 187, 47, 469, 16328, 28750]\n",
      "[5623, 13694, 15817, 22550, 54, 64, 32887, 6740, 5216, 985]\n",
      "[510, 18308, 7428, 949, 253, 1264, 1896, 15050, 285, 7418]\n",
      "[14868, 13004, 27488, 13, 1379, 247, 1007, 2708, 323, 2714]\n",
      "[1672, 7426, 27, 3586, 4079, 302, 4450, 1192, 1634, 19270]\n",
      "[688, 619, 1390, 1501, 13, 309, 369, 25534, 281, 923]\n",
      "[41, 32007, 473, 8765, 310, 247, 30108, 1981, 9168, 4441]\n",
      "[50, 27, 187, 187, 21381, 15736, 502, 74, 5321, 3239]\n",
      "[38, 23044, 10156, 33769, 14943, 327, 6277, 657, 781, 13]\n",
      "[44, 511, 5973, 657, 4576, 416, 26, 15, 18, 21326]\n",
      "[10297, 682, 1229, 187, 187, 34, 1384, 27, 740, 313]\n",
      "[38126, 46581, 323, 686, 12784, 304, 9231, 38883, 5373, 8]\n",
      "[16621, 15553, 18435, 27, 187, 187, 14316, 521, 8920, 275]\n",
      "[23475, 8916, 50000, 17156, 19121, 4163, 2145, 187, 187, 2993]\n",
      "[47, 1123, 72, 9325, 372, 1445, 1950, 36828, 187, 187]\n",
      "[7910, 5783, 1966, 2219, 273, 4255, 43093, 46269, 275, 13338]\n",
      "[25989, 32161, 1169, 5315, 2697, 6956, 75, 1123, 884, 15]\n",
      "[35473, 23488, 374, 25654, 36, 187, 187, 1276, 310, 247]\n",
      "[7456, 14, 48, 5692, 264, 35052, 313, 8683, 34, 10]\n",
      "[37, 13537, 25351, 428, 401, 4306, 37635, 187, 187, 38863]\n",
      "[3122, 187, 475, 8283, 4332, 13, 416, 1751, 443, 2555]\n",
      "[1276, 310, 253, 3486, 273, 2393, 38890, 21781, 327, 253]\n",
      "[37, 2068, 83, 27529, 265, 5402, 21726, 496, 3364, 327]\n",
      "[70, 14, 5719, 436, 281, 247, 3331, 1163, 187, 187]\n",
      "[5305, 1934, 27, 15657, 5171, 14896, 13874, 1934, 17844, 8461]\n",
      "[36, 335, 2767, 29336, 16291, 187, 187, 2809, 11282, 556]\n",
      "[8732, 399, 15, 367, 3592, 460, 80, 362, 15, 3096]\n",
      "[27270, 10471, 8698, 5874, 68, 4121, 285, 20577, 5418, 4714]\n",
      "[18744, 2652, 386, 2205, 671, 1728, 433, 285, 574, 1264]\n",
      "[6623, 255, 2914, 990, 13, 7870, 2216, 285, 13943, 10581]\n",
      "[6, 8490, 6740, 16, 17921, 16, 4793, 16, 1342, 16]\n",
      "[6682, 8893, 27, 362, 18, 187, 10008, 27, 329, 7518]\n",
      "[9301, 5018, 8767, 323, 2255, 342, 14086, 25960, 187, 187]\n",
      "[52, 6062, 13978, 3548, 626, 253, 760, 27606, 2341, 364]\n",
      "[20794, 4175, 46005, 275, 611, 11028, 284, 31513, 1108, 41547]\n",
      "[10241, 4, 418, 14375, 54, 7669, 6061, 19052, 38706, 1852]\n",
      "[1628, 1552, 1806, 753, 1457, 2816, 7717, 3420, 8406, 5119]\n",
      "[38997, 273, 9601, 7974, 668, 28331, 6729, 556, 6726, 562]\n",
      "[13745, 9857, 21508, 187, 187, 1552, 310, 247, 9386, 3790]\n",
      "[14277, 5581, 187, 187, 6930, 187, 475, 1214, 7582, 418]\n",
      "[10859, 20224, 571, 27, 329, 407, 7509, 273, 253, 3998]\n",
      "[1992, 2794, 247, 747, 11846, 25906, 436, 4254, 310, 908]\n",
      "[44, 1279, 70, 4255, 13, 5686, 611, 1279, 70, 30542]\n",
      "[10851, 47407, 753, 253, 3416, 476, 320, 2326, 11922, 247]\n",
      "[5696, 38169, 4651, 13, 10954, 3819, 17068, 272, 273, 13603]\n",
      "[37881, 12516, 28110, 8913, 327, 530, 15, 52, 904, 444]\n",
      "[16, 21723, 424, 16, 187, 3122, 50254, 50254, 50254, 8480]\n",
      "[35, 736, 339, 24703, 36304, 407, 308, 3544, 187, 187]\n",
      "[1231, 264, 24037, 8330, 27, 14482, 353, 12653, 2325, 24547]\n",
      "[510, 12899, 273, 7682, 5240, 1050, 1204, 27, 17907, 275]\n",
      "[510, 4050, 48660, 6696, 273, 27993, 496, 10083, 12664, 423]\n",
      "[26788, 13, 4223, 3285, 13, 4332, 187, 187, 9263, 3184]\n",
      "[4, 20396, 657, 6117, 2449, 15691, 187, 4, 25329, 27]\n",
      "[510, 3975, 273, 24709, 20745, 868, 187, 187, 3039, 14468]\n",
      "[11862, 37051, 947, 187, 187, 6067, 1113, 21601, 398, 403]\n",
      "[1532, 187, 1590, 27, 32134, 10840, 187, 10383, 27, 13119]\n",
      "[2019, 10723, 1503, 335, 2364, 313, 59, 335, 2364, 15]\n",
      "[10697, 428, 18, 428, 18, 428, 740, 45916, 24549, 608]\n",
      "[42, 6699, 2952, 1264, 273, 47179, 15350, 1390, 2360, 15]\n",
      "[510, 1921, 326, 309, 369, 3240, 11561, 1309, 253, 1390]\n",
      "[17967, 13247, 16901, 8, 20391, 3970, 80, 4913, 444, 328]\n",
      "[49210, 4487, 273, 3094, 38526, 24221, 466, 418, 15, 8302]\n",
      "[27315, 31049, 327, 253, 6115, 7021, 457, 84, 37029, 27069]\n",
      "[18281, 436, 1501, 187, 187, 10516, 281, 1501, 187, 187]\n",
      "[23, 9067, 9776, 9491, 187, 1542, 4673, 17327, 187, 13720]\n",
      "[12547, 27, 12843, 11643, 187, 187, 7910, 1107, 273, 7875]\n",
      "[510, 1219, 3163, 66, 3928, 27424, 8273, 369, 11420, 275]\n",
      "[56, 74, 15723, 12318, 2679, 187, 187, 4257, 2561, 556]\n",
      "[50, 27, 187, 187, 2347, 281, 873, 8459, 273, 10882]\n",
      "[34721, 273, 9758, 7806, 301, 366, 285, 36082, 273, 32772]\n",
      "[24349, 2577, 9915, 556, 644, 6276, 247, 2143, 1685, 13]\n",
      "[11778, 368, 2455, 2959, 271, 21558, 432, 634, 3858, 281]\n",
      "[15035, 966, 10030, 4697, 35325, 551, 187, 50274, 36073, 9]\n",
      "[8732, 416, 1135, 797, 434, 13631, 8212, 7963, 273, 28651]\n",
      "[15035, 1159, 8083, 29, 53, 8743, 1051, 19598, 27, 6618]\n",
      "[6189, 77, 569, 187, 187, 46, 6968, 7432, 21128, 187]\n",
      "[21691, 187, 187, 1328, 17157, 7143, 7394, 6016, 326, 310]\n",
      "[3122, 187, 475, 8283, 4267, 14, 14952, 10550, 15, 681]\n",
      "[3122, 29021, 3859, 4966, 20396, 31993, 42495, 15691, 187, 44707]\n",
      "[4041, 273, 253, 954, 49322, 272, 3533, 275, 8118, 14]\n",
      "[26399, 29410, 187, 187, 1147, 806, 5485, 275, 4059, 323]\n",
      "[56, 9482, 1400, 444, 2293, 1830, 310, 247, 19908, 42250]\n",
      "[998, 1032, 2726, 13662, 37357, 187, 187, 30971, 2891, 9061]\n",
      "[11752, 6924, 8033, 12234, 187, 187, 11752, 6924, 8033, 12234]\n",
      "[12612, 11583, 41, 273, 253, 40035, 7306, 1022, 3162, 2789]\n",
      "[22784, 16261, 13, 4104, 187, 187, 9707, 10197, 5757, 3481]\n",
      "[11185, 187, 187, 48698, 14, 46, 1432, 454, 6131, 9058]\n",
      "[35, 41265, 40782, 27529, 387, 3719, 5377, 187, 187, 10195]\n",
      "[5683, 44275, 247, 2045, 1304, 13, 33070, 2065, 1641, 12248]\n",
      "[48, 16279, 38716, 2761, 2456, 3330, 12395, 13, 534, 1537]\n",
      "[3122, 15256, 18669, 187, 50274, 20968, 313, 68, 10, 4332]\n",
      "[4967, 309, 2615, 626, 21050, 323, 18694, 5582, 30926, 285]\n",
      "[40268, 1346, 452, 8058, 326, 1014, 253, 954, 16907, 15377]\n",
      "[1276, 359, 6311, 432, 8216, 434, 46667, 3958, 187, 187]\n",
      "[688, 247, 3668, 3727, 13, 9812, 14, 9638, 14, 14231]\n",
      "[44, 10614, 1608, 39837, 6505, 27334, 78, 1596, 825, 15071]\n",
      "[19932, 4165, 3783, 37509, 27, 7335, 302, 5145, 15556, 261]\n",
      "[4257, 16428, 27, 346, 6504, 1374, 342, 473, 23839, 5489]\n",
      "[53, 2697, 41, 5472, 34, 187, 187, 2214, 247, 3158]\n",
      "[4125, 436, 310, 367, 3269, 3235, 604, 2455, 2712, 369]\n",
      "[50254, 50276, 36, 2185, 4708, 3481, 24730, 187, 50254, 50276]\n",
      "[18656, 7685, 49395, 35230, 17792, 411, 9581, 1198, 46681, 330]\n",
      "[18281, 436, 27, 187, 187, 49, 16606, 1879, 684, 5262]\n",
      "[187, 1857, 31107, 15, 2051, 15, 40625, 313, 16134, 10]\n",
      "[11144, 264, 23701, 968, 187, 187, 14422, 12726, 457, 84]\n",
      "[44906, 73, 45377, 457, 84, 7586, 13, 1219, 1988, 13]\n",
      "[26845, 367, 187, 20443, 1722, 13, 5215, 5498, 3053, 521]\n",
      "[30952, 32735, 11685, 31, 187, 14219, 1457, 10099, 6781, 187]\n",
      "[28549, 13, 4596, 884, 13, 4332, 187, 187, 22098, 1041]\n",
      "[27970, 6258, 40605, 26006, 187, 187, 2042, 368, 878, 4957]\n",
      "[1044, 1443, 41467, 35578, 84, 10396, 17504, 14564, 313, 4350]\n",
      "[2347, 513, 368, 5451, 6505, 32, 313, 34, 4092, 1953]\n",
      "[19174, 27, 187, 34, 7138, 14, 1826, 13, 30732, 13]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_all_tokens2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 41\u001b[0m, in \u001b[0;36mcompile_all_tokens2\u001b[0;34m(sequence_length, batch_size, max_batches, device, save_dir)\u001b[0m\n\u001b[1;32m     38\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mmax_batches)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m pile_dedup_sample:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokens[:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Get as many sequence_length chunks as possible from tokens\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2635\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2598\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2618\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2619\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2620\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2622\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2634\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2635\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3054\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3045\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3046\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3047\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3052\u001b[0m )\n\u001b[0;32m-> 3054\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:613\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    591\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    611\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    612\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 613\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:539\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 539\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    551\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    553\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    563\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–‹         | 13/200 [00:18<01:34,  1.99it/s]"
     ]
    }
   ],
   "source": [
    "a = compile_all_tokens2(max_batches=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IPython - skipped argparse\n"
     ]
    }
   ],
   "source": [
    "default_cfg = {\n",
    "    \"seed\": 49,\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_mult\": 128,\n",
    "    \"lr\": 5e-5,\n",
    "    \"num_tokens\": 400_000_000,\n",
    "    \"l1_coeff\": 2,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.999,\n",
    "    \"d_in\": base_model.cfg.d_model,\n",
    "    \"dict_size\": 768*8*2,\n",
    "    \"seq_len\": 256,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"model_name\": \"pythia-160m-deduped\",\n",
    "    \"site\": \"resid_pre\",\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"model_batch_size\": 4,\n",
    "    \"log_every\": 100,\n",
    "    \"save_every\": 30000,\n",
    "    \"dec_init_norm\": 0.08,\n",
    "    \"hook_point\": \"blocks.5.hook_resid_pre\",\n",
    "    \"wandb_project\": \"crosscoder-fun\",\n",
    "    \"wandb_run_name\": \"toy-run-0\",\n",
    "}\n",
    "cfg = arg_parse_update_cfg(default_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "torch.Size([4, 256, 768])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbuffer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Buffer\n\u001b[0;32m----> 4\u001b[0m newb  \u001b[38;5;241m=\u001b[39m \u001b[43mBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_mid_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactual_toks\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/crosscoder_fun/buffer.py:29\u001b[0m, in \u001b[0;36mBuffer.__init__\u001b[0;34m(self, cfg, model_A, model_B, all_tokens)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens \u001b[38;5;241m=\u001b[39m all_tokens\n\u001b[0;32m---> 29\u001b[0m estimated_norm_scaling_factor_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_norm_scaling_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_A\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m estimated_norm_scaling_factor_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimate_norm_scaling_factor(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], model_B)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalisation_factor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     33\u001b[0m [\n\u001b[1;32m     34\u001b[0m     estimated_norm_scaling_factor_A,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/crosscoder_fun/buffer.py:56\u001b[0m, in \u001b[0;36mBuffer.estimate_norm_scaling_factor\u001b[0;34m(self, batch_size, model, n_batches_for_norm_estimate)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# TODO: maybe drop BOS here\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(acts\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     norms_per_batch\u001b[38;5;241m.\u001b[39mappend(acts\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     58\u001b[0m mean_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(norms_per_batch)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from buffer import Buffer\n",
    "newb  = Buffer(cfg, base_model, checkpoint_mid_model, actual_toks[:128])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256*128*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IPython - skipped argparse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating norm scaling factor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 25.58it/s]\n",
      "Estimating norm scaling factor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00, 25.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:01<00:00, 11.92it/s]\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtim_hua\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/crosscoder_fun/wandb/run-20250120_053927-w438h2ng</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim_hua/crosscoder-fun/runs/w438h2ng' target=\"_blank\">toy-run-0</a></strong> to <a href='https://wandb.ai/tim_hua/crosscoder-fun' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim_hua/crosscoder-fun' target=\"_blank\">https://wandb.ai/tim_hua/crosscoder-fun</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim_hua/crosscoder-fun/runs/w438h2ng' target=\"_blank\">https://wandb.ai/tim_hua/crosscoder-fun/runs/w438h2ng</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/3125000 [00:00<17:31:17, 49.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1196.2391357421875, 'l2_loss': 1196.2391357421875, 'l1_loss': 82.25203704833984, 'l0_loss': 6148.953125, 'l1_coeff': 0.0, 'lr': 5e-05, 'explained_variance': -0.14152106642723083, 'explained_variance_A': -0.3090880513191223, 'explained_variance_B': -0.044093161821365356}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 60/3125000 [00:00<9:44:19, 89.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 24.68it/s]\n",
      "  0%|          | 111/3125000 [00:01<10:54:54, 79.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 445.2242736816406, 'l2_loss': 444.2995300292969, 'l1_loss': 722.4590454101562, 'l0_loss': 11339.421875, 'l1_coeff': 0.00128, 'lr': 5e-05, 'explained_variance': 0.590605616569519, 'explained_variance_A': 0.45075660943984985, 'explained_variance_B': 0.6657016277313232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 121/3125000 [00:01<10:28:34, 82.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.91it/s]\n",
      "  0%|          | 188/3125000 [00:02<11:01:52, 78.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.59it/s]\n",
      "  0%|          | 216/3125000 [00:03<16:28:43, 52.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 374.96240234375, 'l2_loss': 372.26580810546875, 'l1_loss': 1053.35693359375, 'l0_loss': 11583.421875, 'l1_coeff': 0.00256, 'lr': 5e-05, 'explained_variance': 0.7047267556190491, 'explained_variance_A': 0.6066105961799622, 'explained_variance_B': 0.7531558275222778}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 246/3125000 [00:04<11:24:30, 76.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.98it/s]\n",
      "  0%|          | 307/3125000 [00:05<11:02:26, 78.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 282.74884033203125, 'l2_loss': 277.85748291015625, 'l1_loss': 1273.7899169921875, 'l0_loss': 11831.6328125, 'l1_coeff': 0.00384, 'lr': 5e-05, 'explained_variance': 0.7681727409362793, 'explained_variance_A': 0.681261420249939, 'explained_variance_B': 0.8019849061965942}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.25it/s]\n",
      "  0%|          | 376/3125000 [00:06<10:44:19, 80.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 19.10it/s]\n",
      "  0%|          | 415/3125000 [00:07<13:34:31, 63.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 223.29476928710938, 'l2_loss': 216.51205444335938, 'l1_loss': 1324.7491455078125, 'l0_loss': 11907.7265625, 'l1_coeff': 0.00512, 'lr': 5e-05, 'explained_variance': 0.8006809949874878, 'explained_variance_A': 0.7192561626434326, 'explained_variance_B': 0.8477643728256226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 434/3125000 [00:07<12:07:57, 71.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.02it/s]\n",
      "  0%|          | 494/3125000 [00:08<11:57:18, 72.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 208.29763793945312, 'l2_loss': 198.7939910888672, 'l1_loss': 1484.944580078125, 'l0_loss': 11846.265625, 'l1_coeff': 0.0064, 'lr': 5e-05, 'explained_variance': 0.8416138887405396, 'explained_variance_A': 0.782340943813324, 'explained_variance_B': 0.8581240177154541}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.71it/s]\n",
      "  0%|          | 562/3125000 [00:09<11:19:41, 76.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.24it/s]\n",
      "  0%|          | 610/3125000 [00:10<12:53:57, 67.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 170.83360290527344, 'l2_loss': 159.59698486328125, 'l1_loss': 1463.1014404296875, 'l0_loss': 11847.7890625, 'l1_coeff': 0.00768, 'lr': 5e-05, 'explained_variance': 0.8532363176345825, 'explained_variance_A': 0.7932829260826111, 'explained_variance_B': 0.8856824636459351}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 620/3125000 [00:10<11:43:41, 74.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.32it/s]\n",
      "  0%|          | 690/3125000 [00:11<10:32:49, 82.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 19.87it/s]\n",
      "  0%|          | 718/3125000 [00:12<15:18:49, 56.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 143.1224365234375, 'l2_loss': 129.58837890625, 'l1_loss': 1510.497802734375, 'l0_loss': 11795.6796875, 'l1_coeff': 0.00896, 'lr': 5e-05, 'explained_variance': 0.8798946142196655, 'explained_variance_A': 0.8330066204071045, 'explained_variance_B': 0.9060719013214111}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 746/3125000 [00:13<11:35:50, 74.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.55it/s]\n",
      "  0%|          | 815/3125000 [00:14<10:46:11, 80.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 128.8684539794922, 'l2_loss': 113.14756774902344, 'l1_loss': 1535.242431640625, 'l0_loss': 11729.953125, 'l1_coeff': 0.01024, 'lr': 5e-05, 'explained_variance': 0.8980711698532104, 'explained_variance_A': 0.8574055433273315, 'explained_variance_B': 0.9199135303497314}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.47it/s]\n",
      "  0%|          | 874/3125000 [00:15<11:15:50, 77.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.26it/s]\n",
      "  0%|          | 914/3125000 [00:16<13:46:02, 63.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 110.98519134521484, 'l2_loss': 93.09612274169922, 'l1_loss': 1552.87060546875, 'l0_loss': 11711.8984375, 'l1_coeff': 0.01152, 'lr': 5e-05, 'explained_variance': 0.9132499694824219, 'explained_variance_A': 0.8797569274902344, 'explained_variance_B': 0.9307528734207153}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 944/3125000 [00:16<12:13:52, 70.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.16it/s]\n",
      "  0%|          | 1003/3125000 [00:17<12:37:25, 68.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 108.40829467773438, 'l2_loss': 88.49311828613281, 'l1_loss': 1555.87353515625, 'l0_loss': 11600.8984375, 'l1_coeff': 0.0128, 'lr': 5e-05, 'explained_variance': 0.9183162450790405, 'explained_variance_A': 0.8912547826766968, 'explained_variance_B': 0.9322361350059509}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 19.03it/s]\n",
      "  0%|          | 1069/3125000 [00:18<11:39:54, 74.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.62it/s]\n",
      "  0%|          | 1118/3125000 [00:19<13:13:28, 65.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 105.28305053710938, 'l2_loss': 82.33505249023438, 'l1_loss': 1629.8292236328125, 'l0_loss': 11448.546875, 'l1_coeff': 0.01408, 'lr': 5e-05, 'explained_variance': 0.9319586753845215, 'explained_variance_A': 0.9131718873977661, 'explained_variance_B': 0.937177300453186}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1128/3125000 [00:20<11:57:43, 72.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.80it/s]\n",
      "  0%|          | 1187/3125000 [00:21<11:17:33, 76.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.66it/s]\n",
      "  0%|          | 1218/3125000 [00:21<15:18:02, 56.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 95.05500030517578, 'l2_loss': 69.89958190917969, 'l1_loss': 1637.72265625, 'l0_loss': 11321.4296875, 'l1_coeff': 0.01536, 'lr': 5e-05, 'explained_variance': 0.9398634433746338, 'explained_variance_A': 0.9261484146118164, 'explained_variance_B': 0.9451087713241577}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1258/3125000 [00:22<10:41:56, 81.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.96it/s]\n",
      "  0%|          | 1318/3125000 [00:23<11:38:18, 74.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 84.62615966796875, 'l2_loss': 59.290550231933594, 'l1_loss': 1522.572509765625, 'l0_loss': 11218.109375, 'l1_coeff': 0.01664, 'lr': 5e-05, 'explained_variance': 0.9455729722976685, 'explained_variance_A': 0.9297982454299927, 'explained_variance_B': 0.9535121321678162}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 19.15it/s]\n",
      "  0%|          | 1384/3125000 [00:24<9:02:18, 96.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 27.78it/s]\n",
      "  0%|          | 1425/3125000 [00:24<9:58:53, 86.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 76.20369720458984, 'l2_loss': 50.09722900390625, 'l1_loss': 1456.833984375, 'l0_loss': 10993.1015625, 'l1_coeff': 0.01792, 'lr': 5e-05, 'explained_variance': 0.9531610012054443, 'explained_variance_A': 0.9407783150672913, 'explained_variance_B': 0.9591221213340759}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1439/3125000 [00:25<8:57:25, 96.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 14.50it/s]\n",
      "  0%|          | 1506/3125000 [00:26<11:39:48, 74.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 88.17947387695312, 'l2_loss': 58.55598068237305, 'l1_loss': 1542.89013671875, 'l0_loss': 10851.21875, 'l1_coeff': 0.0192, 'lr': 5e-05, 'explained_variance': 0.9521947503089905, 'explained_variance_A': 0.9452762603759766, 'explained_variance_B': 0.9498292803764343}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 23.53it/s]\n",
      "  0%|          | 1571/3125000 [00:27<9:00:15, 96.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 24.74it/s]\n",
      "  0%|          | 1611/3125000 [00:27<10:31:27, 82.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 76.43123626708984, 'l2_loss': 46.04279708862305, 'l1_loss': 1483.8106689453125, 'l0_loss': 10791.984375, 'l1_coeff': 0.02048, 'lr': 5e-05, 'explained_variance': 0.9579405784606934, 'explained_variance_A': 0.9513286352157593, 'explained_variance_B': 0.9610964059829712}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1633/3125000 [00:28<10:40:50, 81.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.16it/s]\n",
      "  0%|          | 1695/3125000 [00:29<12:12:59, 71.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.96it/s]\n",
      "  0%|          | 1712/3125000 [00:30<22:53:00, 37.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 78.07183074951172, 'l2_loss': 45.22510528564453, 'l1_loss': 1509.500244140625, 'l0_loss': 10602.6171875, 'l1_coeff': 0.02176, 'lr': 5e-05, 'explained_variance': 0.9614866971969604, 'explained_variance_A': 0.9585089683532715, 'explained_variance_B': 0.959269642829895}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1761/3125000 [00:30<11:33:28, 75.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.24it/s]\n",
      "  0%|          | 1819/3125000 [00:32<11:49:48, 73.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 79.27752685546875, 'l2_loss': 44.69355773925781, 'l1_loss': 1501.04052734375, 'l0_loss': 10418.9296875, 'l1_coeff': 0.02304, 'lr': 5e-05, 'explained_variance': 0.964844822883606, 'explained_variance_A': 0.9624900817871094, 'explained_variance_B': 0.9608471393585205}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.82it/s]\n",
      "  0%|          | 1886/3125000 [00:33<11:01:23, 78.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.93it/s]\n",
      "  0%|          | 1916/3125000 [00:33<15:30:01, 55.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 72.38909912109375, 'l2_loss': 36.85110855102539, 'l1_loss': 1461.265869140625, 'l0_loss': 10219.90625, 'l1_coeff': 0.02432, 'lr': 5e-05, 'explained_variance': 0.9678746461868286, 'explained_variance_A': 0.9654660224914551, 'explained_variance_B': 0.9672067165374756}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1944/3125000 [00:34<11:43:42, 73.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.89it/s]\n",
      "  0%|          | 2013/3125000 [00:35<10:46:53, 80.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 84.3740005493164, 'l2_loss': 47.00244140625, 'l1_loss': 1459.8265380859375, 'l0_loss': 10101.1796875, 'l1_coeff': 0.0256, 'lr': 5e-05, 'explained_variance': 0.9704200029373169, 'explained_variance_A': 0.9699584245681763, 'explained_variance_B': 0.9558425545692444}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.40it/s]\n",
      "  0%|          | 2072/3125000 [00:36<11:38:08, 74.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.65it/s]\n",
      "  0%|          | 2119/3125000 [00:37<12:57:49, 66.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 75.4444580078125, 'l2_loss': 37.754051208496094, 'l1_loss': 1402.1728515625, 'l0_loss': 9886.4921875, 'l1_coeff': 0.02688, 'lr': 5e-05, 'explained_variance': 0.9743948578834534, 'explained_variance_A': 0.9730702638626099, 'explained_variance_B': 0.9664196968078613}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2138/3125000 [00:37<11:11:38, 77.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.36it/s]\n",
      "  0%|          | 2196/3125000 [00:38<12:12:03, 71.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 65.39859008789062, 'l2_loss': 29.175479888916016, 'l1_loss': 1286.33203125, 'l0_loss': 9749.90625, 'l1_coeff': 0.02816, 'lr': 5e-05, 'explained_variance': 0.9732116460800171, 'explained_variance_A': 0.9710004329681396, 'explained_variance_B': 0.9735487699508667}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.03it/s]\n",
      "  0%|          | 2265/3125000 [00:40<11:08:57, 77.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.42it/s]\n",
      "  0%|          | 2314/3125000 [00:41<13:01:23, 66.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 74.16581726074219, 'l2_loss': 31.922100067138672, 'l1_loss': 1434.908935546875, 'l0_loss': 9565.921875, 'l1_coeff': 0.02944, 'lr': 5e-05, 'explained_variance': 0.9778450727462769, 'explained_variance_A': 0.979667067527771, 'explained_variance_B': 0.9699521064758301}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2323/3125000 [00:41<12:05:19, 71.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.78it/s]\n",
      "  0%|          | 2390/3125000 [00:42<11:13:29, 77.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.72it/s]\n",
      "  0%|          | 2419/3125000 [00:43<15:37:35, 55.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 64.96277618408203, 'l2_loss': 26.388341903686523, 'l1_loss': 1255.67822265625, 'l0_loss': 9403.28125, 'l1_coeff': 0.03072, 'lr': 5e-05, 'explained_variance': 0.975999116897583, 'explained_variance_A': 0.9741057753562927, 'explained_variance_B': 0.9762933254241943}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2456/3125000 [00:43<12:48:53, 67.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.76it/s]\n",
      "  0%|          | 2510/3125000 [00:44<12:14:55, 70.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 62.888763427734375, 'l2_loss': 23.43719482421875, 'l1_loss': 1232.8614501953125, 'l0_loss': 9296.375, 'l1_coeff': 0.032, 'lr': 5e-05, 'explained_variance': 0.9782857894897461, 'explained_variance_A': 0.977840781211853, 'explained_variance_B': 0.9777477979660034}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.52it/s]\n",
      "  0%|          | 2581/3125000 [00:46<10:33:38, 82.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 23.87it/s]\n",
      "  0%|          | 2619/3125000 [00:46<11:07:45, 77.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 62.67619705200195, 'l2_loss': 22.9033203125, 'l1_loss': 1195.0985107421875, 'l0_loss': 9069.921875, 'l1_coeff': 0.03328, 'lr': 5e-05, 'explained_variance': 0.9788900017738342, 'explained_variance_A': 0.97809898853302, 'explained_variance_B': 0.9786554574966431}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2633/3125000 [00:46<9:31:55, 90.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 20.35it/s]\n",
      "  0%|          | 2705/3125000 [00:47<10:53:26, 79.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 62.2692985534668, 'l2_loss': 21.223968505859375, 'l1_loss': 1187.654296875, 'l0_loss': 8928.1953125, 'l1_coeff': 0.03456, 'lr': 5e-05, 'explained_variance': 0.9806457757949829, 'explained_variance_A': 0.9801451563835144, 'explained_variance_B': 0.9803112745285034}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.48it/s]\n",
      "  0%|          | 2763/3125000 [00:48<11:49:26, 73.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.97it/s]\n",
      "  0%|          | 2811/3125000 [00:49<12:26:58, 69.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 59.92285919189453, 'l2_loss': 19.132579803466797, 'l1_loss': 1138.12158203125, 'l0_loss': 8729.6484375, 'l1_coeff': 0.03584, 'lr': 5e-05, 'explained_variance': 0.982133150100708, 'explained_variance_A': 0.9802696108818054, 'explained_variance_B': 0.9824930429458618}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2831/3125000 [00:50<10:37:54, 81.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.82it/s]\n",
      "  0%|          | 2891/3125000 [00:51<11:15:53, 76.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.97it/s]\n",
      "  0%|          | 2911/3125000 [00:51<19:58:40, 43.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 66.20458984375, 'l2_loss': 20.71979522705078, 'l1_loss': 1225.3448486328125, 'l0_loss': 8626.7734375, 'l1_coeff': 0.03712, 'lr': 5e-05, 'explained_variance': 0.9829503893852234, 'explained_variance_A': 0.9829859733581543, 'explained_variance_B': 0.98067307472229}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2954/3125000 [00:52<11:02:39, 78.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.76it/s]\n",
      "  0%|          | 3012/3125000 [00:53<11:47:37, 73.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 65.09286499023438, 'l2_loss': 20.955669403076172, 'l1_loss': 1149.406005859375, 'l0_loss': 8505.3515625, 'l1_coeff': 0.0384, 'lr': 5e-05, 'explained_variance': 0.9807083606719971, 'explained_variance_A': 0.9803259372711182, 'explained_variance_B': 0.9803898334503174}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3021/3125000 [00:53<11:52:37, 73.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.35it/s]\n",
      "  0%|          | 3078/3125000 [00:55<14:21:04, 60.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.37it/s]\n",
      "  0%|          | 3115/3125000 [00:55<15:15:24, 56.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 76.85281372070312, 'l2_loss': 23.97357940673828, 'l1_loss': 1332.6419677734375, 'l0_loss': 8292.4453125, 'l1_coeff': 0.03968, 'lr': 5e-05, 'explained_variance': 0.9840060472488403, 'explained_variance_A': 0.9869582653045654, 'explained_variance_B': 0.9769103527069092}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3144/3125000 [00:56<11:18:32, 76.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.18it/s]\n",
      "  0%|          | 3211/3125000 [00:57<10:53:17, 79.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 64.2777099609375, 'l2_loss': 20.178346633911133, 'l1_loss': 1076.6446533203125, 'l0_loss': 8149.4453125, 'l1_coeff': 0.04096, 'lr': 5e-05, 'explained_variance': 0.9812037944793701, 'explained_variance_A': 0.9790123701095581, 'explained_variance_B': 0.9819427728652954}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.90it/s]\n",
      "  0%|          | 3270/3125000 [00:58<11:20:42, 76.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.27it/s]\n",
      "  0%|          | 3319/3125000 [00:59<13:00:51, 66.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 62.970916748046875, 'l2_loss': 17.99297523498535, 'l1_loss': 1064.818603515625, 'l0_loss': 8010.140625, 'l1_coeff': 0.04224, 'lr': 5e-05, 'explained_variance': 0.9832982420921326, 'explained_variance_A': 0.981763482093811, 'explained_variance_B': 0.9837067127227783}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3330/3125000 [00:59<11:32:40, 75.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.90it/s]\n",
      "  0%|          | 3399/3125000 [01:00<10:37:10, 81.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 75.37968444824219, 'l2_loss': 26.67446517944336, 'l1_loss': 1119.145751953125, 'l0_loss': 7913.953125, 'l1_coeff': 0.04352, 'lr': 5e-05, 'explained_variance': 0.9829422831535339, 'explained_variance_A': 0.9826867580413818, 'explained_variance_B': 0.975132405757904}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 15.66it/s]\n",
      "  0%|          | 3460/3125000 [01:02<11:24:27, 76.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.84it/s]\n",
      "  0%|          | 3510/3125000 [01:03<12:24:18, 69.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 67.4717788696289, 'l2_loss': 18.937278747558594, 'l1_loss': 1083.359375, 'l0_loss': 7736.890625, 'l1_coeff': 0.0448, 'lr': 5e-05, 'explained_variance': 0.9843300580978394, 'explained_variance_A': 0.9831665754318237, 'explained_variance_B': 0.9829293489456177}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3521/3125000 [01:03<11:08:07, 77.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.50it/s]\n",
      "  0%|          | 3590/3125000 [01:04<10:52:33, 79.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 16.68it/s]\n",
      "  0%|          | 3619/3125000 [01:05<16:14:48, 53.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 66.74794006347656, 'l2_loss': 17.36458396911621, 'l1_loss': 1071.6873779296875, 'l0_loss': 7593.5859375, 'l1_coeff': 0.04608, 'lr': 5e-05, 'explained_variance': 0.9849069118499756, 'explained_variance_A': 0.9839762449264526, 'explained_variance_B': 0.9842144250869751}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3648/3125000 [01:05<11:35:29, 74.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 19.05it/s]\n",
      "  0%|          | 3712/3125000 [01:06<11:23:55, 76.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 63.79872512817383, 'l2_loss': 17.244823455810547, 'l1_loss': 982.9793701171875, 'l0_loss': 7455.9140625, 'l1_coeff': 0.04736, 'lr': 5e-05, 'explained_variance': 0.9838117361068726, 'explained_variance_A': 0.9811895489692688, 'explained_variance_B': 0.9848067760467529}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.45it/s]\n",
      "  0%|          | 3779/3125000 [01:08<14:11:49, 61.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 19.79it/s]\n",
      "  0%|          | 3816/3125000 [01:09<14:12:42, 61.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 66.96625518798828, 'l2_loss': 18.287511825561523, 'l1_loss': 1000.7964477539062, 'l0_loss': 7367.0078125, 'l1_coeff': 0.04864, 'lr': 5e-05, 'explained_variance': 0.9830179810523987, 'explained_variance_A': 0.9810871481895447, 'explained_variance_B': 0.9835548400878906}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3837/3125000 [01:09<11:07:13, 77.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 14.56it/s]\n",
      "  0%|          | 3902/3125000 [01:10<13:27:54, 64.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 66.82257080078125, 'l2_loss': 17.41907501220703, 'l1_loss': 989.6533203125, 'l0_loss': 7222.4921875, 'l1_coeff': 0.04992, 'lr': 5e-05, 'explained_variance': 0.9840904474258423, 'explained_variance_A': 0.9820393323898315, 'explained_variance_B': 0.9846476316452026}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 12.93it/s]\n",
      "  0%|          | 3966/3125000 [01:12<15:05:27, 57.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 14.24it/s]\n",
      "  0%|          | 4019/3125000 [01:13<12:52:52, 67.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 74.26235961914062, 'l2_loss': 17.999221801757812, 'l1_loss': 1098.889404296875, 'l0_loss': 7132.4140625, 'l1_coeff': 0.0512, 'lr': 5e-05, 'explained_variance': 0.9853003025054932, 'explained_variance_A': 0.9854974746704102, 'explained_variance_B': 0.9838504791259766}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4029/3125000 [01:13<11:43:51, 73.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17.95it/s]\n",
      "  0%|          | 4088/3125000 [01:14<11:24:34, 75.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 20.23it/s]\n",
      "  0%|          | 4117/3125000 [01:15<16:34:32, 52.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 66.85575866699219, 'l2_loss': 16.087154388427734, 'l1_loss': 967.3894653320312, 'l0_loss': 6987.875, 'l1_coeff': 0.05248, 'lr': 5e-05, 'explained_variance': 0.9853802919387817, 'explained_variance_A': 0.9834029674530029, 'explained_variance_B': 0.9860292673110962}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4154/3125000 [01:16<15:06:05, 57.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 13.59it/s]\n",
      "  0%|          | 4219/3125000 [01:17<11:02:11, 78.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 71.39600372314453, 'l2_loss': 18.13732147216797, 'l1_loss': 990.6749267578125, 'l0_loss': 6850.9453125, 'l1_coeff': 0.05376, 'lr': 5e-05, 'explained_variance': 0.9853328466415405, 'explained_variance_A': 0.9837422370910645, 'explained_variance_B': 0.9842305183410645}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 18.88it/s]\n",
      "  0%|          | 4283/3125000 [01:18<9:58:05, 86.96it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 32.48it/s]\n",
      "  0%|          | 4322/3125000 [01:18<10:01:08, 86.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 64.45153045654297, 'l2_loss': 15.300433158874512, 'l1_loss': 893.0069580078125, 'l0_loss': 6742.8046875, 'l1_coeff': 0.05504, 'lr': 5e-05, 'explained_variance': 0.9854601621627808, 'explained_variance_A': 0.9827118515968323, 'explained_variance_B': 0.9864538908004761}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4336/3125000 [01:19<8:53:47, 97.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 14.83it/s]\n",
      "  0%|          | 4403/3125000 [01:20<14:48:29, 58.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 72.37870788574219, 'l2_loss': 17.371700286865234, 'l1_loss': 976.6868896484375, 'l0_loss': 6639.3984375, 'l1_coeff': 0.05632, 'lr': 5e-05, 'explained_variance': 0.9848951101303101, 'explained_variance_A': 0.983425498008728, 'explained_variance_B': 0.9847656488418579}\n",
      "Refreshing the buffer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      "  0%|          | 4409/3125000 [01:20<15:52:50, 54.58it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/crosscoder-model-diff-replication/checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/crosscoder_fun/trainer.py:76\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtrange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps):\n\u001b[0;32m---> 76\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_every\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/crosscoder_fun/trainer.py:42\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m     acts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrosscoder\u001b[38;5;241m.\u001b[39mget_losses(acts)\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/crosscoder_fun/buffer.py:112\u001b[0m, in \u001b[0;36mBuffer.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointer \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/crosscoder_fun/buffer.py:84\u001b[0m, in \u001b[0;36mBuffer.refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m cache_A: ActivationCache\n\u001b[0;32m---> 84\u001b[0m _, cache_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_B\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhook_point\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m cache_B: ActivationCache\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:657\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03mIf return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03museful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;124;03mactivations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 657\u001b[0m out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/transformer_lens/hook_points.py:568\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    563\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    564\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    565\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    566\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    567\u001b[0m ):\n\u001b[0;32m--> 568\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:575\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    571\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    572\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    573\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/transformer_lens/components/abstract_attention.py:208\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    207\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_rot_q(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_rotary(q, kv_cache_pos_offset, attention_mask))\n\u001b[0;32m--> 208\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_rot_k\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_rotary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# keys are cached so no offset\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mfloat32, torch\u001b[38;5;241m.\u001b[39mfloat64]:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# If using 16 bits, increase the precision to avoid numerical instabilities\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1741\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1741\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1742\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m cfg \u001b[38;5;241m=\u001b[39m arg_parse_update_cfg(default_cfg)\n\u001b[1;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(cfg, base_model, checkpoint_mid_model, actual_toks)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n",
      "File \u001b[0;32m~/crosscoder_fun/trainer.py:82\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/crosscoder_fun/trainer.py:70\u001b[0m, in \u001b[0;36mTrainer.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrosscoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/crosscoder_fun/crosscoder.py:143\u001b[0m, in \u001b[0;36mCrossCoder.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_save_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     weight_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m     cfg_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cfg.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/crosscoder_fun/crosscoder.py:131\u001b[0m, in \u001b[0;36mCrossCoder.create_save_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_save_dir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    128\u001b[0m     base_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/crosscoder-model-diff-replication/checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m     version_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mint\u001b[39m(file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(file)\n\u001b[1;32m    133\u001b[0m     ]\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(version_list):\n\u001b[1;32m    135\u001b[0m         version \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmax\u001b[39m(version_list)\n",
      "File \u001b[0;32m/opt/conda/envs/newenv/lib/python3.11/pathlib.py:931\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    928\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_child_relpath(name)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspace/crosscoder-model-diff-replication/checkpoints'"
     ]
    }
   ],
   "source": [
    "default_cfg = {\n",
    "    \"seed\": 49,\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_mult\": 128,\n",
    "    \"lr\": 5e-5,\n",
    "    \"num_tokens\": 400_000_000,\n",
    "    \"l1_coeff\": 2,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.999,\n",
    "    \"d_in\": base_model.cfg.d_model,\n",
    "    \"dict_size\": 768*8*2,\n",
    "    \"seq_len\": 256,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"model_name\": \"pythia-160m-deduped\",\n",
    "    \"site\": \"resid_pre\",\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"model_batch_size\": 4,\n",
    "    \"log_every\": 100,\n",
    "    \"save_every\": 30000,\n",
    "    \"dec_init_norm\": 0.08,\n",
    "    \"hook_point\": \"blocks.5.hook_resid_pre\",\n",
    "    \"wandb_project\": \"crosscoder-fun\",\n",
    "    \"wandb_run_name\": \"toy-run-0\",\n",
    "}\n",
    "cfg = arg_parse_update_cfg(default_cfg)\n",
    "\n",
    "trainer = Trainer(cfg, base_model, checkpoint_mid_model, actual_toks)\n",
    "trainer.train()\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_files\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatures\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdownload_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdownload_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverification_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVerificationMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_infos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Load a dataset from the Hugging Face Hub, or a local dataset.\n",
      "\n",
      "You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].\n",
      "\n",
      "A dataset is a directory that contains some data files in generic formats (JSON, CSV, Parquet, etc.) and possibly\n",
      "in a generic structure (Webdataset, ImageFolder, AudioFolder, VideoFolder, etc.)\n",
      "\n",
      "This function does the following under the hood:\n",
      "\n",
      "    1. Load a dataset builder:\n",
      "\n",
      "        * Find the most common data format in the dataset and pick its associated builder (JSON, CSV, Parquet, Webdataset, ImageFolder, AudioFolder, etc.)\n",
      "        * Find which file goes into which split (e.g. train/test) based on file and directory names or on the YAML configuration\n",
      "        * It is also possible to specify `data_files` manually, and which dataset builder to use (e.g. \"parquet\").\n",
      "\n",
      "    2. Run the dataset builder:\n",
      "\n",
      "        In the general case:\n",
      "\n",
      "        * Download the data files from the dataset if they are not already available locally or cached.\n",
      "        * Process and cache the dataset in typed Arrow tables for caching.\n",
      "\n",
      "            Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n",
      "            They can be directly accessed from disk, loaded in RAM or even streamed over the web.\n",
      "\n",
      "        In the streaming case:\n",
      "\n",
      "        * Don't download or cache anything. Instead, the dataset is lazily loaded and will be streamed on-the-fly when iterating on it.\n",
      "\n",
      "    3. Return a dataset built from the requested splits in `split` (default: all).\n",
      "\n",
      "It can also use a custom dataset builder if the dataset contains a dataset script, but this feature is mostly for backward compatibility.\n",
      "In this case the dataset script file must be named after the dataset repository or directory and end with \".py\".\n",
      "\n",
      "Args:\n",
      "\n",
      "    path (`str`):\n",
      "        Path or name of the dataset.\n",
      "\n",
      "        - if `path` is a dataset repository on the HF hub (list all available datasets with [`huggingface_hub.list_datasets`])\n",
      "          -> load the dataset from supported files in the repository (csv, json, parquet, etc.)\n",
      "          e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing the data files.\n",
      "\n",
      "        - if `path` is a local directory\n",
      "          -> load the dataset from supported files in the directory (csv, json, parquet, etc.)\n",
      "          e.g. `'./path/to/directory/with/my/csv/data'`.\n",
      "\n",
      "        - if `path` is the name of a dataset builder and `data_files` or `data_dir` is specified\n",
      "          (available builders are \"json\", \"csv\", \"parquet\", \"arrow\", \"text\", \"xml\", \"webdataset\", \"imagefolder\", \"audiofolder\", \"videofolder\")\n",
      "          -> load the dataset from the files in `data_files` or `data_dir`\n",
      "          e.g. `'parquet'`.\n",
      "\n",
      "        It can also point to a local dataset script but this is not recommended.\n",
      "    name (`str`, *optional*):\n",
      "        Defining the name of the dataset configuration.\n",
      "    data_dir (`str`, *optional*):\n",
      "        Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\n",
      "        the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n",
      "    data_files (`str` or `Sequence` or `Mapping`, *optional*):\n",
      "        Path(s) to source data file(s).\n",
      "    split (`Split` or `str`):\n",
      "        Which split of the data to load.\n",
      "        If `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\n",
      "        If given, will return a single Dataset.\n",
      "        Splits can be combined and specified like in tensorflow-datasets.\n",
      "    cache_dir (`str`, *optional*):\n",
      "        Directory to read/write data. Defaults to `\"~/.cache/huggingface/datasets\"`.\n",
      "    features (`Features`, *optional*):\n",
      "        Set the features type to use for this dataset.\n",
      "    download_config ([`DownloadConfig`], *optional*):\n",
      "        Specific download configuration parameters.\n",
      "    download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n",
      "        Download/generate mode.\n",
      "    verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n",
      "        Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).\n",
      "\n",
      "        <Added version=\"2.9.1\"/>\n",
      "    keep_in_memory (`bool`, defaults to `None`):\n",
      "        Whether to copy the dataset in-memory. If `None`, the dataset\n",
      "        will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n",
      "        nonzero. See more details in the [improve performance](../cache#improve-performance) section.\n",
      "    save_infos (`bool`, defaults to `False`):\n",
      "        Save the dataset information (checksums/size/splits/...).\n",
      "    revision ([`Version`] or `str`, *optional*):\n",
      "        Version of the dataset script to load.\n",
      "        As datasets have their own git repository on the Datasets Hub, the default version \"main\" corresponds to their \"main\" branch.\n",
      "        You can specify a different version than the default \"main\" by using a commit SHA or a git tag of the dataset repository.\n",
      "    token (`str` or `bool`, *optional*):\n",
      "        Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "        If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
      "    streaming (`bool`, defaults to `False`):\n",
      "        If set to `True`, don't download the data files. Instead, it streams the data progressively while\n",
      "        iterating on the dataset. An [`IterableDataset`] or [`IterableDatasetDict`] is returned instead in this case.\n",
      "\n",
      "        Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\n",
      "        Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\n",
      "        like rar and xz are not yet supported. The tgz format doesn't allow streaming.\n",
      "    num_proc (`int`, *optional*, defaults to `None`):\n",
      "        Number of processes when downloading and generating the dataset locally.\n",
      "        Multiprocessing is disabled by default.\n",
      "\n",
      "        <Added version=\"2.7.0\"/>\n",
      "    storage_options (`dict`, *optional*, defaults to `None`):\n",
      "        **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n",
      "\n",
      "        <Added version=\"2.11.0\"/>\n",
      "    trust_remote_code (`bool`, defaults to `False`):\n",
      "        Whether or not to allow for datasets defined on the Hub using a dataset script. This option\n",
      "        should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      "        execute code present on the Hub on your local machine.\n",
      "\n",
      "        <Added version=\"2.16.0\"/>\n",
      "\n",
      "        <Changed version=\"2.20.0\">\n",
      "\n",
      "        `trust_remote_code` defaults to `False` if not specified.\n",
      "\n",
      "        </Changed>\n",
      "\n",
      "    **config_kwargs (additional keyword arguments):\n",
      "        Keyword arguments to be passed to the `BuilderConfig`\n",
      "        and used in the [`DatasetBuilder`].\n",
      "\n",
      "Returns:\n",
      "    [`Dataset`] or [`DatasetDict`]:\n",
      "    - if `split` is not `None`: the dataset requested,\n",
      "    - if `split` is `None`, a [`~datasets.DatasetDict`] with each split.\n",
      "\n",
      "    or [`IterableDataset`] or [`IterableDatasetDict`]: if `streaming=True`\n",
      "\n",
      "    - if `split` is not `None`, the dataset is requested\n",
      "    - if `split` is `None`, a [`~datasets.streaming.IterableDatasetDict`] with each split.\n",
      "\n",
      "Example:\n",
      "\n",
      "Load a dataset from the Hugging Face Hub:\n",
      "\n",
      "```py\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('cornell-movie-review-data/rotten_tomatoes', split='train')\n",
      "\n",
      "# Load a subset or dataset configuration (here 'sst2')\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('nyu-mll/glue', 'sst2', split='train')\n",
      "\n",
      "# Manual mapping of data files to splits\n",
      ">>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n",
      ">>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)\n",
      "\n",
      "# Manual selection of a directory to load\n",
      ">>> ds = load_dataset('namespace/your_dataset_name', data_dir='folder_name')\n",
      "```\n",
      "\n",
      "Load a local dataset:\n",
      "\n",
      "```py\n",
      "# Load a CSV file\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')\n",
      "\n",
      "# Load a JSON file\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')\n",
      "\n",
      "# Load from a local loading script (not recommended)\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')\n",
      "```\n",
      "\n",
      "Load an [`~datasets.IterableDataset`]:\n",
      "\n",
      "```py\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('cornell-movie-review-data/rotten_tomatoes', split='train', streaming=True)\n",
      "```\n",
      "\n",
      "Load an image dataset with the `ImageFolder` dataset builder:\n",
      "\n",
      "```py\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      /opt/conda/envs/newenv/lib/python3.11/site-packages/datasets/load.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "?load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
